# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_data.ipynb.

# %% auto 0
__all__ = ['Webpage', 'get_page_all', 'get_all_links']

# %% ../nbs/01_data.ipynb 2
import warnings
warnings.filterwarnings('ignore')
import requests
from bs4 import BeautifulSoup
import enchant
import re
import random
from collections import Counter
from fastai.text.all import *

# %% ../nbs/01_data.ipynb 6
class Webpage:
    def __init__(self, url):
        self.url = url
        self.html = ""
        self.links = []
        self.text = []
        self.cleaned_text = []
        self.most_common_words = []

    def get_html(self, timeout = 5):
        user_agents = [ 
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36', 
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36', 
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36', 
            'Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148', 
            'Mozilla/5.0 (Linux; Android 11; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.72 Mobile Safari/537.36' 
        ] 
        user_agent = random.choice(user_agents) 
        headers = {'User-Agent': user_agent} 
        page = requests.get(self.url, timeout=timeout, headers=headers)
        self.html = BeautifulSoup(page.text, "html.parser")

    def get_html_anchors(self, keyword="http"):
        for anchor in self.html.findAll('a'):
            link = anchor.get('href')
            if link == None or link == "":
                continue
            if keyword in link:
                self.links.append(link)
                
    def get_html_text(self, tags=["p"]):
        for tag in tags:
            for p in self.html.findAll(tag):
                p_text = p.getText().strip()
                if p_text == None or p_text == '':
                    continue
                self.text.append(p_text)

    def clean_text(self, enchant_dict="en_US"):
        rx = "[^a-zA-Z0-9 ]+"
        all_text = ' '.join(self.text)
        regex_text = re.sub(rx,'',all_text).strip()
        split = regex_text.split()
        if enchant_dict != "": d = enchant.Dict(enchant_dict)
        for word in split:
            if enchant_dict == "":
                self.cleaned_text.append(word)
            elif d.check(word): 
                self.cleaned_text.append(word)

    def k_common_words(self, k=10, ignore=["the","to","of","and","a","in","on","is","for","by"]):
        if self.cleaned_text == "":
            text = self.text
        else:
            text = self.cleaned_text
        all_text = ' '.join(text).lower()
        split = all_text.split()
        split_ignore = [word for word in split if word not in ignore]
        counts = Counter(split_ignore)
        k_most_common = counts.most_common(k)
        self.most_common_words = k_most_common

# %% ../nbs/01_data.ipynb 10
def get_page_all(url, k, ignore_words):
    page = Webpage(url)
    page.get_html()
    page.get_html_text(tags=["p","h1","h2","h3","span"])
    page.clean_text()
    page.k_common_words(k=k, ignore=ignore_words)
    page.get_html_anchors()
    return page

def get_all_links(url, dict, k, min_text_len=50, ignore_words=[], ignore_filenames=[".mp3",".jpg",".png"]):
    page = get_page_all(url, k, ignore_words)
    text = ' '.join(page.cleaned_text)
    dict[url] = [text, page.most_common_words]
    print(url,"Contains",len(page.links),"Links")

    for link in page.links:
        if all(x not in link for x in ignore_filenames):
            try:
                page = get_page_all(link, k, ignore_words)
                text = ' '.join(page.cleaned_text)
                if len(text) < min_text_len: continue
                dict[link] = [text, page.most_common_words]
            except:
                pass
