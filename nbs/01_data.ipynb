{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> Web scraping and tools for data collection and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import enchant\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "from fastai.text.all import *\n",
    "import hashlib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility Function to Check GPU Status\n",
    "def check_gpu():\n",
    "    print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    if num_devices > 0:\n",
    "        for device in range(0,num_devices):\n",
    "            print(\"Device\", device, \"|\", torch.cuda.get_device_name(device), \n",
    "            \"| Allocated:\", round(torch.cuda.memory_allocated(device)/1024**3,1), \"GB\",\n",
    "            \"| Cached:\", round(torch.cuda.memory_reserved(device)/1024**3,1), \"GB\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Device 0 | NVIDIA GeForce RTX 3050 Ti Laptop GPU | Allocated: 0.0 GB | Cached: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "check_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Webpage:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.hash = self.get_hash_str()\n",
    "        self.requested = False\n",
    "        self.page_text = \"\"\n",
    "        self.html = \"\"\n",
    "        self.links = []\n",
    "        self.text = []\n",
    "        self.cleaned_text = []\n",
    "        self.most_common_words = []\n",
    "    \n",
    "    def get_page(self, headers, min_size, max_size):\n",
    "        r = requests.get(self.url, stream=True, headers=headers)\n",
    "        content_length = int(r.headers.get('Content-Length', 0))\n",
    "        data = []\n",
    "        length = 0\n",
    "\n",
    "        if content_length > max_size:\n",
    "            return None\n",
    "\n",
    "        for chunk in r.iter_content(1024):\n",
    "            data.append(chunk)\n",
    "            length += len(chunk)\n",
    "            if length > max_size:\n",
    "                return None\n",
    "        r._content = b''.join(data)\n",
    "        if len(r.text) < min_size: return None\n",
    "        return r.text\n",
    "\n",
    "    def get_page_html(self, min_size=1000, max_size=2000000):\n",
    "        user_agents = [ \n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36', \n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36', \n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36', \n",
    "            'Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148', \n",
    "            'Mozilla/5.0 (Linux; Android 11; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.72 Mobile Safari/537.36' \n",
    "        ] \n",
    "        user_agent = random.choice(user_agents) \n",
    "        headers = {'User-Agent': user_agent} \n",
    "        self.page_text = self.get_page(headers, min_size, max_size)\n",
    "        self.html = BeautifulSoup(self.page_text, \"html.parser\")\n",
    "        self.requested = True\n",
    "\n",
    "    def get_hash_str(self, inp=\"\"):\n",
    "        return hashlib.sha3_256((self.url+inp).encode()).hexdigest()\n",
    "\n",
    "    def get_html_anchors(self, keyword=\"http\"):\n",
    "        for anchor in self.html.findAll('a'):\n",
    "            link = anchor.get('href')\n",
    "            if link == None or link == \"\":\n",
    "                continue\n",
    "            if keyword in link:\n",
    "                self.links.append(link)\n",
    "                \n",
    "    def get_html_text(self, tags=[\"p\"]):\n",
    "        for tag in tags:\n",
    "            for p in self.html.findAll(tag):\n",
    "                p_text = p.getText().strip()\n",
    "                if p_text == None or p_text == '':\n",
    "                    continue\n",
    "                self.text.append(p_text)\n",
    "\n",
    "    def clean_html_text(self, max_words, enchant_dict=\"en_US\", ignore=[], min_word_len=2):\n",
    "        rx = \"[^a-zA-Z0-9 ]+\"\n",
    "        all_text = ' '.join(self.text).lower()\n",
    "        regex_text = re.sub(rx,'',all_text).strip()\n",
    "        split = regex_text.split()\n",
    "        split = [word for word in split if word not in ignore]\n",
    "        if enchant_dict != \"\": d = enchant.Dict(enchant_dict)\n",
    "        for word in split:\n",
    "            if len(self.cleaned_text) >= max_words: break\n",
    "            if len(word) > min_word_len:\n",
    "                if enchant_dict == \"\":\n",
    "                    self.cleaned_text.append(word)\n",
    "                elif d.check(word): \n",
    "                    self.cleaned_text.append(word)\n",
    "\n",
    "    def k_common_words(self, k=10, ignore=[\"the\",\"to\",\"of\",\"and\",\"a\",\"in\",\"on\",\"is\",\"for\",\"by\"]):\n",
    "        if self.cleaned_text == \"\":\n",
    "            text = self.text\n",
    "        else:\n",
    "            text = self.cleaned_text\n",
    "        all_text = ' '.join(text).lower()\n",
    "        split = all_text.split()\n",
    "        split_ignore = [word for word in split if word not in ignore]\n",
    "        counts = Counter(split_ignore)\n",
    "        k_most_common = counts.most_common(k)\n",
    "        self.most_common_words = k_most_common\n",
    "\n",
    "    def save_text(self, path, fname):\n",
    "        file = open(path+fname, 'wb')\n",
    "        pickle.dump(self.cleaned_text, file)\n",
    "        file.close()\n",
    "\n",
    "    def load_text(self, path, fname):\n",
    "        file = open(path+fname, 'rb')\n",
    "        self.cleaned_text = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    def save_links(self, path, fname):\n",
    "        file = open(path+fname, 'wb')\n",
    "        pickle.dump(self.links, file)\n",
    "        file.close()\n",
    "\n",
    "    def load_links(self, path, fname):\n",
    "        file = open(path+fname, 'rb')\n",
    "        self.links = pickle.load(file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 most common English words\n"
     ]
    }
   ],
   "source": [
    "url = \"https://gist.githubusercontent.com/deekayen/4148741/raw/98d35708fa344717d8eee15d11987de6c8e26d7d/1-1000.txt\"\n",
    "common_english = Webpage(url)\n",
    "common_english.get_page_html(min_size=1000)\n",
    "english_words = common_english.html.getText().lower()\n",
    "english_words = english_words.split('\\n')\n",
    "print(len(english_words),\"most common English words\")\n",
    "#english_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_sources = [\"http://www.ageofautism.com/\",\n",
    " \"http://www.naturalnews.com\", \n",
    " \"https://foodbabe.com/starthere/\",\n",
    " \"http://www.chopra.com\",\n",
    " \"https://www.mercola.com/\",\n",
    " \"https://www.history.com/\",\n",
    " \"https://doctoroz.com/\",\n",
    " \"https://www.disclose.tv/\",\n",
    " \"https://christiananswers.net/\",\n",
    " \"https://heartland.org/\"]\n",
    "\n",
    "science_sources = [\"https://sciencebasedmedicine.org/\",\n",
    " \"https://www.hopkinsmedicine.org/gim/research/method/ebm.html\",\n",
    " \"https://www.bbc.com/news/science_and_environment\",\n",
    " \"https://www.nature.com/\",\n",
    " \"https://www.science.org/\",\n",
    " \"https://www.snopes.com/top/\",\n",
    " \"https://quackwatch.org/\",\n",
    " \"https://www.skepdic.com/\",\n",
    " \"http://scibabe.com/\",\n",
    " \"http://pandasthumb.org/\",\n",
    " \"https://skepticalscience.com/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = science_sources[7]\n",
    "path = os.getcwd()+'/data/'\n",
    "if os.path.isdir(path) is False: os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_page = Webpage(url)\n",
    "test_page.get_page_html()\n",
    "test_page.get_html_text()\n",
    "test_page.get_html_anchors()\n",
    "test_page.clean_html_text(500)\n",
    "test_page.save_text(path, test_page.hash+'.text')\n",
    "test_page.save_links(path, test_page.hash+'.links')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Text\n",
      "Loading Links\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'skeptics',\n",
       " 'dictionary',\n",
       " 'features',\n",
       " 'definitions',\n",
       " 'arguments',\n",
       " 'and',\n",
       " 'essays',\n",
       " 'hundreds',\n",
       " 'strange',\n",
       " 'beliefs',\n",
       " 'amusing',\n",
       " 'deceptions',\n",
       " 'and',\n",
       " 'dangerous',\n",
       " 'delusions',\n",
       " 'also',\n",
       " 'features',\n",
       " 'dozens',\n",
       " 'entries',\n",
       " 'logical',\n",
       " 'fallacies',\n",
       " 'cognitive',\n",
       " 'biases',\n",
       " 'perception',\n",
       " 'science',\n",
       " 'and',\n",
       " 'philosophy',\n",
       " 'also',\n",
       " 'posted',\n",
       " 'are',\n",
       " 'over',\n",
       " 'years',\n",
       " 'reader',\n",
       " 'comments',\n",
       " 'date',\n",
       " 'status',\n",
       " 'entry',\n",
       " 'reader',\n",
       " 'comments',\n",
       " 'natural',\n",
       " 'cancer',\n",
       " 'cures',\n",
       " 'revision',\n",
       " 'argument',\n",
       " 'ignorance',\n",
       " 'reader',\n",
       " 'comments',\n",
       " 'reader',\n",
       " 'comments',\n",
       " 'psychokinesis',\n",
       " 'reader',\n",
       " 'comments',\n",
       " 'sample',\n",
       " 'the',\n",
       " 'skeptics',\n",
       " 'dictionary',\n",
       " '1858',\n",
       " 'grotto',\n",
       " 'the',\n",
       " 'river',\n",
       " 'gave',\n",
       " 'near',\n",
       " 'peasant',\n",
       " 'named',\n",
       " 'claimed',\n",
       " 'that',\n",
       " 'the',\n",
       " 'virgin',\n",
       " 'identifying',\n",
       " 'herself',\n",
       " 'the',\n",
       " 'immaculate',\n",
       " 'conception',\n",
       " 'appeared',\n",
       " 'her',\n",
       " 'some',\n",
       " 'think',\n",
       " 'such',\n",
       " 'great',\n",
       " 'number',\n",
       " 'have',\n",
       " 'provided',\n",
       " 'opportunity',\n",
       " 'channel',\n",
       " 'short',\n",
       " 'theological',\n",
       " 'treatise',\n",
       " 'some',\n",
       " 'significance',\n",
       " 'seems',\n",
       " 'however',\n",
       " 'that',\n",
       " 'the',\n",
       " 'main',\n",
       " 'message',\n",
       " 'from',\n",
       " 'the',\n",
       " 'alleged',\n",
       " 'mother',\n",
       " 'god',\n",
       " 'was',\n",
       " 'pray',\n",
       " 'and',\n",
       " 'penance',\n",
       " 'for',\n",
       " 'the',\n",
       " 'conversion',\n",
       " 'the',\n",
       " 'world',\n",
       " 'take',\n",
       " 'drink',\n",
       " 'the',\n",
       " 'spring',\n",
       " 'nutshell',\n",
       " 'gods',\n",
       " 'are',\n",
       " 'beings',\n",
       " 'with',\n",
       " 'unnatural',\n",
       " 'powers',\n",
       " 'who',\n",
       " 'never',\n",
       " 'die',\n",
       " 'some',\n",
       " 'are',\n",
       " 'believed',\n",
       " 'the',\n",
       " 'controllers',\n",
       " 'creators',\n",
       " 'various',\n",
       " 'parts',\n",
       " 'nature',\n",
       " 'many',\n",
       " 'are',\n",
       " 'thought',\n",
       " 'require',\n",
       " 'worship',\n",
       " 'and',\n",
       " 'obedience',\n",
       " 'from',\n",
       " 'humans',\n",
       " 'these',\n",
       " 'gods',\n",
       " 'reward',\n",
       " 'punish',\n",
       " 'depending',\n",
       " 'whether',\n",
       " 'please',\n",
       " 'them',\n",
       " 'stories',\n",
       " 'gods',\n",
       " 'have',\n",
       " 'been',\n",
       " 'told',\n",
       " 'most',\n",
       " 'societies',\n",
       " 'that',\n",
       " 'know',\n",
       " 'going',\n",
       " 'back',\n",
       " 'least',\n",
       " '10000',\n",
       " 'years',\n",
       " 'gods',\n",
       " 'are',\n",
       " 'portrayed',\n",
       " 'beings',\n",
       " 'who',\n",
       " 'never',\n",
       " 'die',\n",
       " 'with',\n",
       " 'mighty',\n",
       " 'powers',\n",
       " 'able',\n",
       " 'make',\n",
       " 'nature',\n",
       " 'what',\n",
       " 'they',\n",
       " 'want',\n",
       " 'most',\n",
       " 'gods',\n",
       " 'are',\n",
       " 'pictured',\n",
       " 'being',\n",
       " 'born',\n",
       " 'and',\n",
       " 'having',\n",
       " 'parents',\n",
       " 'some',\n",
       " 'stories',\n",
       " 'show',\n",
       " 'gods',\n",
       " 'being',\n",
       " 'able',\n",
       " 'change',\n",
       " 'the',\n",
       " 'weather',\n",
       " 'and',\n",
       " 'cause',\n",
       " 'mighty',\n",
       " 'storms',\n",
       " 'floods',\n",
       " 'earthquakes',\n",
       " 'volcanic',\n",
       " 'eruptions',\n",
       " 'for',\n",
       " 'the',\n",
       " 'most',\n",
       " 'part',\n",
       " 'scientists',\n",
       " 'have',\n",
       " 'replaced',\n",
       " 'stories',\n",
       " 'about',\n",
       " 'gods',\n",
       " 'with',\n",
       " 'scientific',\n",
       " 'explanations',\n",
       " 'how',\n",
       " 'the',\n",
       " 'universe',\n",
       " 'was',\n",
       " 'formed',\n",
       " 'and',\n",
       " 'how',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'other',\n",
       " 'living',\n",
       " 'creatures',\n",
       " '2010',\n",
       " 'belief',\n",
       " 'omnipotent',\n",
       " 'omniscient',\n",
       " 'creator',\n",
       " 'the',\n",
       " 'world',\n",
       " 'does',\n",
       " 'not',\n",
       " 'itself',\n",
       " 'have',\n",
       " 'any',\n",
       " 'moral',\n",
       " 'still',\n",
       " 'you',\n",
       " 'decide',\n",
       " 'whether',\n",
       " 'right',\n",
       " 'obey',\n",
       " 'his',\n",
       " 'human',\n",
       " 'beings',\n",
       " 'are',\n",
       " 'anything',\n",
       " 'special',\n",
       " 'are',\n",
       " 'the',\n",
       " 'creatures',\n",
       " 'that',\n",
       " 'must',\n",
       " 'ponder',\n",
       " 'and',\n",
       " 'talk',\n",
       " 'jay',\n",
       " 'jay',\n",
       " '1997',\n",
       " 'essay',\n",
       " 'innocently',\n",
       " 'lack',\n",
       " 'conflict',\n",
       " 'between',\n",
       " 'science',\n",
       " 'and',\n",
       " 'religion',\n",
       " 'arises',\n",
       " 'from',\n",
       " 'lack',\n",
       " 'overlap',\n",
       " 'between',\n",
       " 'their',\n",
       " 'respective',\n",
       " 'domains',\n",
       " 'professional',\n",
       " 'the',\n",
       " 'empirical',\n",
       " 'constitution',\n",
       " 'the',\n",
       " 'universe',\n",
       " 'and',\n",
       " 'religion',\n",
       " 'the',\n",
       " 'search',\n",
       " 'for',\n",
       " 'proper',\n",
       " 'ethical',\n",
       " 'values',\n",
       " 'and',\n",
       " 'the',\n",
       " 'spiritual',\n",
       " 'meaning',\n",
       " 'our',\n",
       " 'lives',\n",
       " 'the',\n",
       " 'attainment',\n",
       " 'wisdom',\n",
       " 'full',\n",
       " 'life',\n",
       " 'requires',\n",
       " 'extensive',\n",
       " 'attention',\n",
       " 'both',\n",
       " 'great',\n",
       " 'book',\n",
       " 'tells',\n",
       " 'that',\n",
       " 'the',\n",
       " 'truth',\n",
       " 'can',\n",
       " 'make',\n",
       " 'free',\n",
       " 'and',\n",
       " 'that',\n",
       " 'will',\n",
       " 'live',\n",
       " 'optimal',\n",
       " 'harmony',\n",
       " 'with',\n",
       " 'our',\n",
       " 'fellows',\n",
       " 'when',\n",
       " 'learn',\n",
       " 'justly',\n",
       " 'love',\n",
       " 'mercy',\n",
       " 'and',\n",
       " 'walk',\n",
       " 'all',\n",
       " 'hell',\n",
       " 'broke',\n",
       " 'jay',\n",
       " '1997',\n",
       " 'essay',\n",
       " 'innocently',\n",
       " 'lack',\n",
       " 'conflict',\n",
       " 'between',\n",
       " 'science',\n",
       " 'and',\n",
       " 'religion',\n",
       " 'arises',\n",
       " 'from',\n",
       " 'lack',\n",
       " 'overlap',\n",
       " 'between',\n",
       " 'their',\n",
       " 'respective',\n",
       " 'domains',\n",
       " 'professional',\n",
       " 'the',\n",
       " 'empirical',\n",
       " 'constitution',\n",
       " 'the',\n",
       " 'universe',\n",
       " 'and',\n",
       " 'religion',\n",
       " 'the',\n",
       " 'search',\n",
       " 'for',\n",
       " 'proper',\n",
       " 'ethical',\n",
       " 'values',\n",
       " 'and',\n",
       " 'the',\n",
       " 'spiritual',\n",
       " 'meaning',\n",
       " 'our',\n",
       " 'lives',\n",
       " 'the',\n",
       " 'attainment',\n",
       " 'wisdom',\n",
       " 'full',\n",
       " 'life',\n",
       " 'requires',\n",
       " 'extensive',\n",
       " 'attention',\n",
       " 'both',\n",
       " 'great',\n",
       " 'book',\n",
       " 'tells',\n",
       " 'that',\n",
       " 'the',\n",
       " 'truth',\n",
       " 'can',\n",
       " 'make',\n",
       " 'free',\n",
       " 'and',\n",
       " 'that',\n",
       " 'will',\n",
       " 'live',\n",
       " 'optimal',\n",
       " 'harmony',\n",
       " 'with',\n",
       " 'our',\n",
       " 'fellows',\n",
       " 'when',\n",
       " 'learn',\n",
       " 'justly',\n",
       " 'love',\n",
       " 'mercy',\n",
       " 'and',\n",
       " 'walk',\n",
       " 'all',\n",
       " 'hell',\n",
       " 'broke',\n",
       " 'jay',\n",
       " '1997',\n",
       " 'essay',\n",
       " 'innocently',\n",
       " 'wrote',\n",
       " 'the',\n",
       " 'lack',\n",
       " 'conflict',\n",
       " 'between',\n",
       " 'science',\n",
       " 'and',\n",
       " 'religion',\n",
       " 'arises',\n",
       " 'from',\n",
       " 'lack',\n",
       " 'overlap',\n",
       " 'between',\n",
       " 'their',\n",
       " 'respective',\n",
       " 'domains',\n",
       " 'professional',\n",
       " 'the',\n",
       " 'empirical',\n",
       " 'constitution',\n",
       " 'the',\n",
       " 'universe',\n",
       " 'and',\n",
       " 'religion',\n",
       " 'the',\n",
       " 'search',\n",
       " 'for',\n",
       " 'proper',\n",
       " 'ethical',\n",
       " 'values',\n",
       " 'and',\n",
       " 'the',\n",
       " 'spiritual',\n",
       " 'meaning',\n",
       " 'our',\n",
       " 'lives',\n",
       " 'the',\n",
       " 'attainment',\n",
       " 'wisdom',\n",
       " 'full',\n",
       " 'life',\n",
       " 'requires',\n",
       " 'extensive',\n",
       " 'attention',\n",
       " 'both',\n",
       " 'great',\n",
       " 'book',\n",
       " 'tells',\n",
       " 'that',\n",
       " 'the',\n",
       " 'truth',\n",
       " 'can',\n",
       " 'make',\n",
       " 'free',\n",
       " 'and',\n",
       " 'that',\n",
       " 'will',\n",
       " 'live',\n",
       " 'optimal',\n",
       " 'harmony',\n",
       " 'with',\n",
       " 'our',\n",
       " 'fellows',\n",
       " 'when',\n",
       " 'learn',\n",
       " 'justly',\n",
       " 'love',\n",
       " 'mercy',\n",
       " 'and',\n",
       " 'walk',\n",
       " 'humbly',\n",
       " 'then',\n",
       " 'all',\n",
       " 'hell',\n",
       " 'broke',\n",
       " 'books',\n",
       " 'ordering',\n",
       " 'information',\n",
       " 'print',\n",
       " 'versions',\n",
       " 'available']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_page = Webpage(url)\n",
    "fname_text = new_page.hash+'.text'\n",
    "fname_links = new_page.hash+'.links'\n",
    "if os.path.isfile(path+fname_text): \n",
    "    new_page.load_text(path, fname_text)\n",
    "    print(\"Loading Text\")\n",
    "else:\n",
    "    new_page.get_page_html()\n",
    "    new_page.get_html_text(tags=[\"p\",\"h1\",\"h2\",\"h3\",\"span\"])\n",
    "    new_page.clean_html_text(500, ignore=english_words[:50])\n",
    "    new_page.save_text(path, fname_text)\n",
    "\n",
    "if os.path.isfile(path+fname_links): \n",
    "    new_page.load_links(path, fname_links)\n",
    "    print(\"Loading Links\")\n",
    "else:\n",
    "    new_page.get_page_html()\n",
    "    new_page.get_html_anchors()\n",
    "    new_page.save_links(path, fname_links)\n",
    "new_page.k_common_words(k=5,ignore=english_words[:50])\n",
    "new_page.cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_page_all(url, k, max_words, ignore_words, path = None):\n",
    "    page = Webpage(url)\n",
    "    fname_text = page.hash+'.text'\n",
    "    fname_links = page.hash+'.links'\n",
    "    if path == None:\n",
    "        page.get_page_html()\n",
    "        page.get_html_text(tags=[\"p\",\"h1\",\"h2\",\"h3\",\"span\"])\n",
    "        page.get_html_anchors()\n",
    "        page.clean_html_text(max_words, ignore=english_words[:50])\n",
    "    else:\n",
    "        if os.path.isfile(path+fname_text): \n",
    "            page.load_text(path, fname_text)\n",
    "        else:\n",
    "            page.get_page_html()\n",
    "            page.get_html_text(tags=[\"p\",\"h1\",\"h2\",\"h3\",\"span\"])\n",
    "            page.clean_html_text(max_words, ignore=english_words[:50])\n",
    "            page.save_text(path, fname_text)\n",
    "\n",
    "        if os.path.isfile(path+fname_links): \n",
    "            page.load_links(path, fname_links)\n",
    "        else:\n",
    "            if page.html == \"\": page.get_page_html()\n",
    "            page.get_html_anchors()\n",
    "            page.save_links(path, fname_links)\n",
    "\n",
    "    if page.cleaned_text is not None:\n",
    "        page.k_common_words(k=k, ignore=ignore_words)\n",
    "    return page\n",
    "\n",
    "def get_all_links(url, dict, k, min_words=20, max_words=500, ignore_words=[], ignore_filenames=[\".mp3\",\".jpg\",\".png\"], max_links=\"\", path=None):\n",
    "    page = get_page_all(url, k, max_words, ignore_words, path)\n",
    "    if page.cleaned_text is not []:\n",
    "        dict[url] = [page.cleaned_text, page.most_common_words]\n",
    "        print(url,\"Contains\",len(page.links),\"Links\")\n",
    "        if max_links == \"\" or max_links > len(page.links): max_links=len(page.links)\n",
    "        \n",
    "        for link in page.links[:max_links]:\n",
    "            if all(x not in link for x in ignore_filenames):\n",
    "                try:\n",
    "                    page = get_page_all(link, k, max_words, ignore_words, path)\n",
    "                    if page.cleaned_text is not []:\n",
    "                        if len(page.cleaned_text) < min_words: continue\n",
    "                        dict[link] = [page.cleaned_text, page.most_common_words]\n",
    "                except:\n",
    "                    pass\n",
    "    else:\n",
    "        print(url,\"returned None, Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30 # words\n",
    "min_words = 20\n",
    "max_words = 1000\n",
    "max_links = \"\"\n",
    "ignore_words = english_words[:100]\n",
    "ignore_filenames = [\".mp3\",\".jpg\",\".png\",\".mp4\",\".jfif\",\"facebook.com\",\"twitter.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#d_pse = {}\n",
    "#get_all_links(pseudo_sources[2], d_pse, k, min_text_len, ignore_words, ignore_filenames)\n",
    "#d_pse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.ageofautism.com/ Contains 705 Links\n",
      "http://www.naturalnews.com Contains 323 Links\n",
      "https://foodbabe.com/starthere/ Contains 126 Links\n",
      "http://www.chopra.com Contains 100 Links\n",
      "https://www.mercola.com/ Contains 125 Links\n",
      "https://www.history.com/ Contains 84 Links\n",
      "https://doctoroz.com/ Contains 28 Links\n",
      "https://www.disclose.tv/ Contains 139 Links\n",
      "https://christiananswers.net/ Contains 9 Links\n",
      "https://heartland.org/ Contains 142 Links\n",
      "https://sciencebasedmedicine.org/ Contains 258 Links\n",
      "https://www.hopkinsmedicine.org/gim/research/method/ebm.html Contains 103 Links\n",
      "https://www.bbc.com/news/science_and_environment Contains 128 Links\n",
      "https://www.nature.com/ Contains 73 Links\n",
      "https://www.science.org/ Contains 29 Links\n",
      "https://www.snopes.com/top/ Contains 35 Links\n",
      "https://quackwatch.org/ Contains 136 Links\n",
      "https://www.skepdic.com/ Contains 103 Links\n",
      "http://scibabe.com/ Contains 118 Links\n",
      "http://pandasthumb.org/ Contains 7 Links\n",
      "https://skepticalscience.com/ Contains 131 Links\n"
     ]
    }
   ],
   "source": [
    "d_pse = {}\n",
    "d_sci = {}\n",
    "path = os.getcwd()+'/data/'\n",
    "if os.path.isdir(path) is False: os.mkdir(path)\n",
    "path_pse = path+'pseudoscience/'\n",
    "path_sci = path+'science/'\n",
    "if os.path.isdir(path_pse) is False: os.mkdir(path_pse)\n",
    "if os.path.isdir(path_sci) is False: os.mkdir(path_sci)\n",
    "\n",
    "for source in pseudo_sources:\n",
    "    get_all_links(source, d_pse, k, min_words, max_words, ignore_words, ignore_filenames, max_links, path_pse)\n",
    "for source in science_sources:\n",
    "    get_all_links(source, d_sci, k, min_words, max_words, ignore_words, ignore_filenames, max_links, path_sci)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Pseudoscience 30 Most Common Words ####\n",
      " [('our', 215), ('information', 185), ('food', 156), ('health', 145), ('twitter', 108), ('2022', 107), ('its', 90), ('donate', 86), ('any', 76), ('autism', 73), ('new', 73), ('read', 68), ('website', 54), ('content', 50), ('policy', 49), ('natural', 49), ('life', 49), ('here', 48), ('email', 47), ('views', 46), ('age', 45), ('free', 45), ('news', 42), ('senate', 42), ('personal', 40), ('vaccine', 38), ('support', 38), ('access', 38), ('heartland', 38), ('proton', 37)] \n",
      "\n",
      "\n",
      "#### Science 30 Most Common Words ####\n",
      " [('2022', 251), ('science', 182), ('read', 159), ('care', 125), ('written', 125), ('share', 122), ('information', 116), ('our', 110), ('medicine', 92), ('och', 84), ('news', 81), ('research', 78), ('new', 76), ('twitter', 72), ('menu', 70), ('why', 62), ('med', 61), ('published', 59), ('johns', 57), ('patients', 57), ('here', 54), ('guidelines', 53), ('updates', 53), ('health', 51), ('man', 51), ('skeptics', 47), ('tweet', 46), ('its', 45), ('testing', 45), ('masks', 42)] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_pse = Counter()\n",
    "count_sci = Counter()\n",
    "for link in d_pse:\n",
    "    count_pse+=Counter(dict(d_pse[link][1]))\n",
    "for link in d_sci:\n",
    "    count_sci+=Counter(dict(d_sci[link][1]))\n",
    "\n",
    "print(\"#### Pseudoscience\",k,\"Most Common Words ####\\n\",count_pse.most_common(k),\"\\n\\n\")\n",
    "print(\"#### Science\",k,\"Most Common Words ####\\n\",count_sci.most_common(k),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in d_pse:\n",
    "    if link in d_sci.keys():\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>common_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://www.ageofautism.com/</th>\n",
       "      <td>during spring kvetched twitter slapped sensitive content label our tweets included link photo before musk stepped see imagine twitter became open again hope twitter exposes censorship many faced especially crackdown mean want see overtly controversial despicable content allowed who defines pornography right know see defender twitter ditches misinformation policy plans reveal internal files free speech suppression effective twitter longer enforcing misleading information policy company adding will soon reveal internal files free speech suppression which could shed light twitters past action...</td>\n",
       "      <td>autism age 2022 our twitter deaths vaccine posted comments its sudden information also epidemic children week died content link policy 0600 perhaps issues take between healthy health current affairs here</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.ageofautism.com/</th>\n",
       "      <td>during spring kvetched twitter slapped sensitive content label our tweets included link photo before musk stepped see imagine twitter became open again hope twitter exposes censorship many faced especially crackdown mean want see overtly controversial despicable content allowed who defines pornography right know see defender twitter ditches misinformation policy plans reveal internal files free speech suppression effective twitter longer enforcing misleading information policy company adding will soon reveal internal files free speech suppression which could shed light twitters past action...</td>\n",
       "      <td>autism age 2022 our twitter deaths vaccine posted comments its sudden information also epidemic children week died content link policy 0600 perhaps issues take between healthy health current affairs here</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.ageofautism.com/donate.html</th>\n",
       "      <td>hello donation autism age tax now secure online donations scroll down their easy form always send paper electronic check well email any time ideas suggestions gentle critiques our cause unknown epidemic sudden deaths 2021 2022 health defense transcend fear blueprint mindful leadership public health real bill gates big global war democracy public health health defense donate click cover buy book shop amazon support recent comments past current contributors connect search donate donate top</td>\n",
       "      <td>health donate defense public hello donation autism age tax secure online donations scroll easy form always send paper electronic check well email any ideas suggestions gentle critiques our cause unknown</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.ageofautism.com/contact-us.html</th>\n",
       "      <td>autism age box 110546 06611 cause unknown epidemic sudden deaths 2021 2022 health defense transcend fear blueprint mindful leadership public health real bill gates big global war democracy public health health defense donate click cover buy book shop amazon support recent comments past current contributors connect search donate contact top</td>\n",
       "      <td>health defense public donate autism age box 110546 06611 cause unknown epidemic sudden deaths 2021 2022 transcend fear blueprint mindful leadership real bill gates big global war democracy click cover</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.ageofautism.com/exclusives.html</th>\n",
       "      <td>editorials series here come elaborate fraud series deer special report epidemiological studies really tell note epidemiological studies here vaccines autism these studies represent most often cited papers scientists public health officials members media trying refute any evidence association between vaccinations autism serious methodological limitations design flaws conflicts interest problems related these studies these flaws been pointed government officials researchers medical review panels even authors studies themselves taken together limitations these studies make impossible conclude...</td>\n",
       "      <td>autism studies health series here public epidemiological vaccines officials limitations flaws study defense donate editorials elaborate fraud deer special report really tell note represent often cited papers scientists members media</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                text  \\\n",
       "http://www.ageofautism.com/                  during spring kvetched twitter slapped sensitive content label our tweets included link photo before musk stepped see imagine twitter became open again hope twitter exposes censorship many faced especially crackdown mean want see overtly controversial despicable content allowed who defines pornography right know see defender twitter ditches misinformation policy plans reveal internal files free speech suppression effective twitter longer enforcing misleading information policy company adding will soon reveal internal files free speech suppression which could shed light twitters past action...   \n",
       "https://www.ageofautism.com/                 during spring kvetched twitter slapped sensitive content label our tweets included link photo before musk stepped see imagine twitter became open again hope twitter exposes censorship many faced especially crackdown mean want see overtly controversial despicable content allowed who defines pornography right know see defender twitter ditches misinformation policy plans reveal internal files free speech suppression effective twitter longer enforcing misleading information policy company adding will soon reveal internal files free speech suppression which could shed light twitters past action...   \n",
       "https://www.ageofautism.com/donate.html                                                                                                                 hello donation autism age tax now secure online donations scroll down their easy form always send paper electronic check well email any time ideas suggestions gentle critiques our cause unknown epidemic sudden deaths 2021 2022 health defense transcend fear blueprint mindful leadership public health real bill gates big global war democracy public health health defense donate click cover buy book shop amazon support recent comments past current contributors connect search donate donate top   \n",
       "https://www.ageofautism.com/contact-us.html                                                                                                                                                                                                                                                                    autism age box 110546 06611 cause unknown epidemic sudden deaths 2021 2022 health defense transcend fear blueprint mindful leadership public health real bill gates big global war democracy public health health defense donate click cover buy book shop amazon support recent comments past current contributors connect search donate contact top   \n",
       "https://www.ageofautism.com/exclusives.html  editorials series here come elaborate fraud series deer special report epidemiological studies really tell note epidemiological studies here vaccines autism these studies represent most often cited papers scientists public health officials members media trying refute any evidence association between vaccinations autism serious methodological limitations design flaws conflicts interest problems related these studies these flaws been pointed government officials researchers medical review panels even authors studies themselves taken together limitations these studies make impossible conclude...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                         common_words  \\\n",
       "http://www.ageofautism.com/                                               autism age 2022 our twitter deaths vaccine posted comments its sudden information also epidemic children week died content link policy 0600 perhaps issues take between healthy health current affairs here   \n",
       "https://www.ageofautism.com/                                              autism age 2022 our twitter deaths vaccine posted comments its sudden information also epidemic children week died content link policy 0600 perhaps issues take between healthy health current affairs here   \n",
       "https://www.ageofautism.com/donate.html                                    health donate defense public hello donation autism age tax secure online donations scroll easy form always send paper electronic check well email any ideas suggestions gentle critiques our cause unknown   \n",
       "https://www.ageofautism.com/contact-us.html                                  health defense public donate autism age box 110546 06611 cause unknown epidemic sudden deaths 2021 2022 transcend fear blueprint mindful leadership real bill gates big global war democracy click cover   \n",
       "https://www.ageofautism.com/exclusives.html  autism studies health series here public epidemiological vaccines officials limitations flaws study defense donate editorials elaborate fraud deer special report really tell note represent often cited papers scientists members media   \n",
       "\n",
       "                                                     label  \n",
       "http://www.ageofautism.com/                  pseudoscience  \n",
       "https://www.ageofautism.com/                 pseudoscience  \n",
       "https://www.ageofautism.com/donate.html      pseudoscience  \n",
       "https://www.ageofautism.com/contact-us.html  pseudoscience  \n",
       "https://www.ageofautism.com/exclusives.html  pseudoscience  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_all = {}\n",
    "for link in d_pse:\n",
    "    text = d_pse[link][0]\n",
    "    if len(text) > max_words: text = text[:max_words]\n",
    "    common_words = ' '.join([count[0] for count in d_pse[link][1]])\n",
    "    if link not in d_all:\n",
    "        d_all[link] = [' '.join(text), common_words, 'pseudoscience']\n",
    "\n",
    "for link in d_sci:\n",
    "    text = d_sci[link][0]\n",
    "    if len(text) > max_words: text = text[:max_words]\n",
    "    common_words = ' '.join([count[0] for count in d_sci[link][1]])\n",
    "    if link not in d_all:\n",
    "        d_all[link] = [' '.join(text), common_words, 'science']\n",
    "\n",
    "df = pd.DataFrame.from_dict(d_all, orient='index', columns=['text', 'common_words', 'label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos och system man den evolution may also study more restricted problem ancestry compare model common ancestry two species unique origin model according which species founded single couple carter 2014 carter xxunk 2019 och xxunk den med den man par man och och den art 7 xxrep 4 0 sedan 1 xxrep 4 0 man men den variation till 5 xxrep 5 0 xxunk 6 xxrep 3 0 till 1 xxrep 4 0 sedan man man genetic modeling human history part xxunk common descent unique origin approaches och genetic modeling human history part unique origin algorithm 2016 med med till discovery institute human origin possible 2019 man sin man par 500 xxrep 3 0 sedan med men 100 xxrep 3 0 discovery institute den den genetic modeling human history part xxunk common descent unique origin approaches 2016 man argue unique origin model where humanity xxunk single couple created diversity xxunk</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos 2022 before musk bought twitter social media giant suspended 11 xxrep 3 0 accounts removed 1 xxrep 5 0 pieces content violating its misinformation policy between 2020 2022 alone now musk owns has made 2022 help medical records more than 1 xxrep 3 0 health care workers interacting directly patients researchers concluded medical masks provide little benefit over respirators specifically researchers 2022 world experiencing strongest decline birth rates over 100 years its unprecedented figure yet except laypersons like xxunk independent researchers xxunk true health official making comments 2022 more than 230 million having received least two shots first time since shots came more vaccinated than persons dying infection just those eligible new boosters 2022 effort xxunk xxunk xxunk article xxunk benefits treatment goes great xxunk xxunk xxunk xxunk does xxunk prevent deaths 2022 now he s finally under xxunk role pandemic censorship went national institute xxunk infectious diseases director</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos continuing browse our site agree our cookies revised privacy policy terms service agree group nations including xxunk digital vaccine read more future xxunk into world where everyone will xxunk technology lower class controlled read more evidence shows unprocessed red meat health risk however vilification natural raw meat products raises potential during victory gardens produced produce many now rediscovering many benefits gardening offers fresh inventory control plan just about everything earth land water minerals plants xxunk food energy genetically engineered foods grain crops like wheat oats barley common sources exposure toxic chemical linked even small amount just these vitamins risk dying even most severe cases drops like rock fact before musk bought twitter social media giant suspended 11 xxrep 3 0 accounts removed 1 xxrep 5 0 pieces content violating its help medical records more than 1 xxrep 3 0 health care workers interacting directly patients researchers concluded world experiencing</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = TextDataLoaders.from_df(df, bs=8, text_col='text', label_col='label')\n",
    "dls.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.905427</td>\n",
       "      <td>0.595364</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.475673</td>\n",
       "      <td>0.553062</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.454600</td>\n",
       "      <td>0.499734</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.370057</td>\n",
       "      <td>0.364939</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.328572</td>\n",
       "      <td>0.322171</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "learn.fine_tune(4, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Device 0 | NVIDIA GeForce RTX 3050 Ti Laptop GPU | Allocated: 0.4 GB | Cached: 2.0 GB\n"
     ]
    }
   ],
   "source": [
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos heartland institute worlds leading think tanks national nonprofit research education organization based xxunk its mission since its founding 1984 discover develop promote solutions social economic problems action xxunk well think xxunk measure our success impact real world heartland institute xxunk essential role national increasingly international movement personal liberty limited government xxunk between freedom xxunk leading xxunk thinkers nations xxunk national state elected officials because effective been subject xxunk criticism even libel various liberal advocacy groups elected officials even please see reply our critics page where answer our critics set record xxunk hope will xxunk comments xxunk websites xxunk conversations friends xxunk staff works board directors 2 xxrep 3 0 xxunk 500 xxunk professional xxunk who serve policy xxunk fellows more than officials who pay xxunk serve our legislative forum heartland staff board directors policy experts heartland institute nonprofit organization xxunk xxunk under section internal xxunk code focus issues education</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos enter term search box find its xxunk xxunk far right xxunk increase xxunk number terms automatically xxunk completely turn feature off archives global warming real leading climate change under xxunk climate skepticism public xxunk misinformation xxunk xxunk xxunk global warming website gets skeptical about global warming skepticism our mission simple xxunk climate misinformation presenting science explaining techniques science denial recently happened upon xxunk detailed article explaining scientific consensus made xxunk even though lot material about consensus xxunk about scientific consensus actually now xxunk created page xxunk xxunk entry which will point our readers towards page small xxunk xxunk xxunk over consensus article xxunk skeptical science full text available below sections about consensus based xxunk xxunk week most likely seen xxunk where term scientific consensus has been xxunk xxunk people example often xxunk xxunk popular opinion think result xxunk determined vote just finding xxunk because opinion xxunk even xxunk xxunk</td>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos cover xxunk xxunk powerful xxunk potential cell engineering ability xxunk xxunk regulatory xxunk enables researchers design xxunk cells advance basic science improve xxunk xxunk now xxunk various cell types groups cells xxunk smart xxunk xxunk xxunk xxunk xxunk xxunk functions see special section beginning page xxunk xxunk online cover smart xxunk xxunk patch blood glucose control insulin xxunk xxunk xxunk essential xxunk blood glucose levels inspired mechanism xxunk developed patch enables xxunk both insulin xxunk xxunk treatment type xxunk xxunk model patch xxunk blood glucose levels within xxunk range xxunk risk xxunk online cover xxunk potential xxunk months cover xxunk xxunk youth which new xxunk xxunk xxunk cells emerging after xxunk infectious challenge xxunk xxunk xxunk red xxunk xxunk xxunk cells join xxunk xxunk gray lack xxunk capacity model xxunk supported new mouse models xxunk separate xxunk discussed focus online cover xxunk replicate xxunk often heavy xxunk lot energy</td>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos guide quackery health fraud intelligent decisions which operated network web sites mailing lists maintained center inquiry sites focus health frauds myths fads fallacies misconduct their main goal provide information difficult impossible get xxunk help visitors special areas interest sites cover autism chiropractic dentistry xxunk marketing many xxunk areas internet health pilot site provides links hundreds reliable health sites contains large library legal cases xxunk board actions government xxunk regulatory actions against questionable medical products these accessed through visit our affiliated sites menu above their contents searched once through page also offer health fraud discussion list more than xxunk members consumer health digest free weekly email newsletter summarizes scientific reports legislative developments enforcement actions information relevant consumer protection consumer its primary focus health occasionally includes scams our homepage hits xxunk million march 2 xxrep 3 0 million august xxunk million 2009 million 2019 new edition most comprehensive text available field</td>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos masks required inside our care facilities vaccinating eligible patients learn boosters additional doses testing patient care visitor guidelines more testing locations masks required inside our care facilities vaccinating eligible patients learn more vaccines boosters additional doses testing patient care visitor guidelines find more testing locations popular searches welcome been our care facilities while will notice few changes since last visit please review page learn about extra safety measures place steps ask take more safely care our patients monitor our communities masks required everyone age inside our care facilities even fully vaccinated boosted against mask will provided need following face coverings allowed gaiters masks exhalation valves clear face masks our staff members may wear additional personal protective equipment depending care providing increase xxunk xxunk omicron xxunk which highly xxunk such masking remains xxunk patients visitors johns medicine xxunk xxunk vaccination status johns medicine locations both clinical xxunk areas because patients</td>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos google has xxunk war independent media has xxunk blocking emails getting our readers recommend free xxunk email receiving service free encrypted email send receive service xxunk continue address natural news world currently live xxunk freedom engineered xxunk experiment xxunk xxunk living fake food social media xxunk xxunk xxunk supply fake xxunk money xxunk xxunk xxunk xxunk xxunk more power read more natural news growing portion people who usually xxunk xxunk xxunk xxunk xxunk xxunk after nearly two years worth zero policy xxunk social media posts xxunk china xxunk xxunk now read more natural news while tens millions people world over long xxunk world economic forum its founder just xxunk global xxunk their xxunk confirmed last interview china state television public xxunk china controlled read more natural news administration energy department official who has generated xxunk controversy legal hot water facing xxunk xxunk news official graduate now xxunk xxunk xxunk</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos today xxunk code red need xxunk xxunk back life many feel like xxunk xxunk room xxunk insights already xxunk xxunk leave our xxunk xxunk setting chaos next xxunk xxunk into political xxunk right thing our time need want xxunk people action more than xxunk xxunk because great ideas never xxunk off light xxunk our xxunk which happened while our nation xxunk pandemic xxunk our nations xxunk learned xxunk politics medicine get solutions why running united states help xxunk problems help heal spent xxunk years xxunk lifetime challenging xxunk comes xxunk xxunk xxunk xxunk looks xxunk xxunk seeking safety their loved ones being xxunk into xxunk expected days xxunk countless people saved broken xxunk xxunk personal xxunk xxunk xxunk give people second xxunk sometimes life changes xxunk making plans growing xxunk xxunk xxunk xxunk xxunk father xxunk xxunk xxunk xxunk xxunk xxunk floor loved country much anyone already here because</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos xxunk xxunk over remarks black xxunk xxunk xxunk dies top group through goal difference xxunk announce joint xxunk best album xxunk xxunk volcano xxunk xxunk volcano xxunk allow police xxunk robots xxunk cost effective way xxunk xxunk xxunk cost effective way xxunk xxunk killed celebrating world cup loss report xxunk 2022 revealed top stories wales xxunk xxunk really xxunk friend says raise concerns over cost new care service people eating pet food says community xxunk 600 energy payment xxunk until after xxunk social media joy need know viral xxunk man xxunk fans video viral xxunk man xxunk fans comes next wales after world cup exit keep latest sport xxunk xxunk world cup xxunk return home wait start historic series xxunk fight talks getting xxunk dies after xxunk xxunk xxunk discover xxunk ancient used eat quiz much know about xxunk xxunk capsule breaks distance record space xxunk xxunk loved xxunk</td>\n",
       "      <td>science</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sources = {'https://infowarslife.com/':'pseudoscience',\n",
    "'https://www.bbc.com/news/':'science',\n",
    "'https://www.dailymail.co.uk/':'pseudoscience',\n",
    "'https://www.si.edu/explore/science':'science',\n",
    "'https://www.foxnews.com/opinion':'pseudoscience',\n",
    "'https://www.disclose.tv/':'pseudoscience',\n",
    "'https://www.snopes.com/top/':'science',\n",
    "'https://www.theskepticsguide.org/about':'science',\n",
    "'https://www.cdc.gov/':'science',\n",
    "'https://www.motherjones.com/':'pseudoscience',\n",
    "'https://www.huffpost.com/':'pseudoscience',\n",
    "'https://arstechnica.com/':'science',\n",
    "'https://nationalreport.net/':'pseudoscience',\n",
    "'https://newspunch.com/':'pseudoscience',\n",
    "'https://www.trunews.com/':'pseudoscience'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: https://www.trunews.com/ 17 words\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>https://infowarslife.com/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.934633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.bbc.com/news/</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.586406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.dailymail.co.uk/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.829928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.si.edu/explore/science</th>\n",
       "      <td>science</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.530939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.foxnews.com/opinion</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.579585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.disclose.tv/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.860695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.snopes.com/top/</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.781662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.theskepticsguide.org/about</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.761140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.cdc.gov/</th>\n",
       "      <td>science</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.724096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.motherjones.com/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.663550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.huffpost.com/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.777269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://arstechnica.com/</th>\n",
       "      <td>science</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.646908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://nationalreport.net/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.901954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://newspunch.com/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.894345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               actual     prediction  \\\n",
       "https://infowarslife.com/               pseudoscience  pseudoscience   \n",
       "https://www.bbc.com/news/                     science        science   \n",
       "https://www.dailymail.co.uk/            pseudoscience  pseudoscience   \n",
       "https://www.si.edu/explore/science            science  pseudoscience   \n",
       "https://www.foxnews.com/opinion         pseudoscience  pseudoscience   \n",
       "https://www.disclose.tv/                pseudoscience  pseudoscience   \n",
       "https://www.snopes.com/top/                   science        science   \n",
       "https://www.theskepticsguide.org/about        science        science   \n",
       "https://www.cdc.gov/                          science  pseudoscience   \n",
       "https://www.motherjones.com/            pseudoscience  pseudoscience   \n",
       "https://www.huffpost.com/               pseudoscience  pseudoscience   \n",
       "https://arstechnica.com/                      science  pseudoscience   \n",
       "https://nationalreport.net/             pseudoscience  pseudoscience   \n",
       "https://newspunch.com/                  pseudoscience  pseudoscience   \n",
       "\n",
       "                                        probability  \n",
       "https://infowarslife.com/                  0.934633  \n",
       "https://www.bbc.com/news/                  0.586406  \n",
       "https://www.dailymail.co.uk/               0.829928  \n",
       "https://www.si.edu/explore/science         0.530939  \n",
       "https://www.foxnews.com/opinion            0.579585  \n",
       "https://www.disclose.tv/                   0.860695  \n",
       "https://www.snopes.com/top/                0.781662  \n",
       "https://www.theskepticsguide.org/about     0.761140  \n",
       "https://www.cdc.gov/                       0.724096  \n",
       "https://www.motherjones.com/               0.663550  \n",
       "https://www.huffpost.com/                  0.777269  \n",
       "https://arstechnica.com/                   0.646908  \n",
       "https://nationalreport.net/                0.901954  \n",
       "https://newspunch.com/                     0.894345  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_pred = {}\n",
    "\n",
    "for source in test_sources:\n",
    "    page = get_page_all(source, k, max_words, ignore_words)\n",
    "    length = len(page.cleaned_text)\n",
    "    if  length < min_words:\n",
    "        print(\"ERROR:\",source,length,\"words\")\n",
    "    else:\n",
    "        common_words = ' '.join([count[0] for count in page.most_common_words])\n",
    "        text = ' '.join(page.cleaned_text)\n",
    "        with learn.no_bar(), learn.no_logging():\n",
    "            prediction = learn.predict(text)\n",
    "        if prediction[0] == \"science\":\n",
    "            p = prediction[2][1].item()\n",
    "        else:\n",
    "            p = prediction[2][0].item()\n",
    "        d_pred[source] = [test_sources[source], prediction[0], p]\n",
    "\n",
    "df = pd.DataFrame.from_dict(d_pred, orient='index', columns=['actual', 'prediction', 'probability'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.save('2022.11.29 Model v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = load_learner('models/2022.11.28 Model.pth', cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pseudometer",
   "language": "python",
   "name": "pseudometer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
