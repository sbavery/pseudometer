{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> Web scraping and tools for data collection and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sbavery/pseudometer/blob/main/nbs/01_data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import enchant\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "from fastai.text.all import *\n",
    "import hashlib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility Function to Check GPU Status\n",
    "def check_gpu():\n",
    "    print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    if num_devices > 0:\n",
    "        for device in range(0,num_devices):\n",
    "            print(\"Device\", device, \"|\", torch.cuda.get_device_name(device), \n",
    "            \"| Allocated:\", round(torch.cuda.memory_allocated(device)/1024**3,1), \"GB\",\n",
    "            \"| Cached:\", round(torch.cuda.memory_reserved(device)/1024**3,1), \"GB\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Device 0 | NVIDIA GeForce RTX 3050 Ti Laptop GPU | Allocated: 0.0 GB | Cached: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "check_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Webpage:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.hash = self.get_hash_str()\n",
    "        self.requested = False\n",
    "        self.page_text = \"\"\n",
    "        self.html = \"\"\n",
    "        self.links = []\n",
    "        self.text = []\n",
    "        self.cleaned_text = []\n",
    "        self.most_common_words = []\n",
    "    \n",
    "    def get_page(self, headers, min_size, max_size):\n",
    "        r = requests.get(self.url, stream=True, headers=headers)\n",
    "        content_length = int(r.headers.get('Content-Length', 0))\n",
    "        data = []\n",
    "        length = 0\n",
    "\n",
    "        if content_length > max_size:\n",
    "            return None\n",
    "\n",
    "        for chunk in r.iter_content(1024):\n",
    "            data.append(chunk)\n",
    "            length += len(chunk)\n",
    "            if length > max_size:\n",
    "                return None\n",
    "        r._content = b''.join(data)\n",
    "        if len(r.text) < min_size: return None\n",
    "        return r.text\n",
    "\n",
    "    def get_page_html(self, min_size=1000, max_size=2000000):\n",
    "        user_agents = [ \n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36', \n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36', \n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36', \n",
    "            'Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148', \n",
    "            'Mozilla/5.0 (Linux; Android 11; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.72 Mobile Safari/537.36' \n",
    "        ] \n",
    "        user_agent = random.choice(user_agents) \n",
    "        headers = {'User-Agent': user_agent} \n",
    "        self.page_text = self.get_page(headers, min_size, max_size)\n",
    "        self.html = BeautifulSoup(self.page_text, \"html.parser\")\n",
    "        self.requested = True\n",
    "\n",
    "    def get_hash_str(self, inp=\"\"):\n",
    "        return hashlib.sha3_256((self.url+inp).encode()).hexdigest()\n",
    "\n",
    "    def get_html_anchors(self, keyword=\"http\"):\n",
    "        for anchor in self.html.findAll('a'):\n",
    "            link = anchor.get('href')\n",
    "            if link == None or link == \"\":\n",
    "                continue\n",
    "            if keyword in link:\n",
    "                self.links.append(link)\n",
    "                \n",
    "    def get_html_text(self, tags=[\"p\"]):\n",
    "        for tag in tags:\n",
    "            for p in self.html.findAll(tag):\n",
    "                p_text = p.getText().strip()\n",
    "                if p_text == None or p_text == '':\n",
    "                    continue\n",
    "                self.text.append(p_text)\n",
    "\n",
    "    def clean_html_text(self, max_words, enchant_dict=\"en_US\", ignore=[], min_word_len=2):\n",
    "        rx = \"[^a-zA-Z0-9 ]+\"\n",
    "        all_text = ' '.join(self.text).lower()\n",
    "        regex_text = re.sub(rx,'',all_text).strip()\n",
    "        split = regex_text.split()\n",
    "        split = [word for word in split if word not in ignore]\n",
    "        if enchant_dict != \"\": d = enchant.Dict(enchant_dict)\n",
    "        for word in split:\n",
    "            if len(self.cleaned_text) >= max_words: break\n",
    "            if len(word) > min_word_len:\n",
    "                if enchant_dict == \"\":\n",
    "                    self.cleaned_text.append(word)\n",
    "                elif d.check(word): \n",
    "                    self.cleaned_text.append(word)\n",
    "\n",
    "    def k_common_words(self, k=10, ignore=[\"the\",\"to\",\"of\",\"and\",\"a\",\"in\",\"on\",\"is\",\"for\",\"by\"]):\n",
    "        if self.cleaned_text == \"\":\n",
    "            text = self.text\n",
    "        else:\n",
    "            text = self.cleaned_text\n",
    "        all_text = ' '.join(text).lower()\n",
    "        split = all_text.split()\n",
    "        split_ignore = [word for word in split if word not in ignore]\n",
    "        counts = Counter(split_ignore)\n",
    "        k_most_common = counts.most_common(k)\n",
    "        self.most_common_words = k_most_common\n",
    "\n",
    "    def save_text(self, path, fname):\n",
    "        file = open(path+fname, 'wb')\n",
    "        pickle.dump(self.cleaned_text, file)\n",
    "        file.close()\n",
    "\n",
    "    def load_text(self, path, fname):\n",
    "        file = open(path+fname, 'rb')\n",
    "        self.cleaned_text = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    def save_links(self, path, fname):\n",
    "        file = open(path+fname, 'wb')\n",
    "        pickle.dump(self.links, file)\n",
    "        file.close()\n",
    "\n",
    "    def load_links(self, path, fname):\n",
    "        file = open(path+fname, 'rb')\n",
    "        self.links = pickle.load(file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 most common English words\n"
     ]
    }
   ],
   "source": [
    "url = \"https://gist.githubusercontent.com/deekayen/4148741/raw/98d35708fa344717d8eee15d11987de6c8e26d7d/1-1000.txt\"\n",
    "common_english = Webpage(url)\n",
    "common_english.get_page_html(min_size=1000)\n",
    "english_words = common_english.html.getText().lower()\n",
    "english_words = english_words.split('\\n')\n",
    "print(len(english_words),\"most common English words\")\n",
    "#english_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_sources = [\"http://www.ageofautism.com/\",\n",
    " \"http://www.naturalnews.com\", \n",
    " \"https://foodbabe.com/starthere/\",\n",
    " \"http://www.chopra.com\",\n",
    " \"https://www.mercola.com/\",\n",
    " \"https://www.history.com/\",\n",
    " \"https://doctoroz.com/\",\n",
    " \"https://www.disclose.tv/\",\n",
    " \"https://christiananswers.net/\",\n",
    " \"https://heartland.org/\",\n",
    " \"https://www.dailymail.co.uk/\",\n",
    " \"https://www.motherjones.com/\"]\n",
    "\n",
    "science_sources = [\"https://sciencebasedmedicine.org/\",\n",
    " \"https://www.hopkinsmedicine.org/gim/research/method/ebm.html\",\n",
    " \"https://www.bbc.com/news/science_and_environment\",\n",
    " \"https://www.nature.com/\",\n",
    " \"https://www.science.org/\",\n",
    " \"https://www.snopes.com/top/\",\n",
    " \"https://quackwatch.org/\",\n",
    " \"https://www.skepdic.com/\",\n",
    " \"http://scibabe.com/\",\n",
    " \"http://pandasthumb.org/\",\n",
    " \"https://skepticalscience.com/\",\n",
    " \"https://www.cdc.gov/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = science_sources[7]\n",
    "path = os.getcwd()+'/data/'\n",
    "if os.path.isdir(path) is False: os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_page = Webpage(url)\n",
    "test_page.get_page_html()\n",
    "test_page.get_html_text()\n",
    "test_page.get_html_anchors()\n",
    "test_page.clean_html_text(500)\n",
    "test_page.save_text(path, test_page.hash+'.text')\n",
    "test_page.save_links(path, test_page.hash+'.links')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Text\n",
      "Loading Links\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'skeptics',\n",
       " 'dictionary',\n",
       " 'features',\n",
       " 'definitions',\n",
       " 'arguments',\n",
       " 'and',\n",
       " 'essays',\n",
       " 'hundreds',\n",
       " 'strange',\n",
       " 'beliefs',\n",
       " 'amusing',\n",
       " 'deceptions',\n",
       " 'and',\n",
       " 'dangerous',\n",
       " 'delusions',\n",
       " 'also',\n",
       " 'features',\n",
       " 'dozens',\n",
       " 'entries',\n",
       " 'logical',\n",
       " 'fallacies',\n",
       " 'cognitive',\n",
       " 'biases',\n",
       " 'perception',\n",
       " 'science',\n",
       " 'and',\n",
       " 'philosophy',\n",
       " 'also',\n",
       " 'posted',\n",
       " 'are',\n",
       " 'over',\n",
       " 'years',\n",
       " 'reader',\n",
       " 'comments',\n",
       " 'date',\n",
       " 'status',\n",
       " 'entry',\n",
       " 'reader',\n",
       " 'comments',\n",
       " 'natural',\n",
       " 'cancer',\n",
       " 'cures',\n",
       " 'revision',\n",
       " 'argument',\n",
       " 'ignorance',\n",
       " 'reader',\n",
       " 'comments',\n",
       " 'reader',\n",
       " 'comments',\n",
       " 'psychokinesis',\n",
       " 'reader',\n",
       " 'comments',\n",
       " 'sample',\n",
       " 'the',\n",
       " 'skeptics',\n",
       " 'dictionary',\n",
       " '1858',\n",
       " 'grotto',\n",
       " 'the',\n",
       " 'river',\n",
       " 'gave',\n",
       " 'near',\n",
       " 'peasant',\n",
       " 'named',\n",
       " 'claimed',\n",
       " 'that',\n",
       " 'the',\n",
       " 'virgin',\n",
       " 'identifying',\n",
       " 'herself',\n",
       " 'the',\n",
       " 'immaculate',\n",
       " 'conception',\n",
       " 'appeared',\n",
       " 'her',\n",
       " 'some',\n",
       " 'think',\n",
       " 'such',\n",
       " 'great',\n",
       " 'number',\n",
       " 'have',\n",
       " 'provided',\n",
       " 'opportunity',\n",
       " 'channel',\n",
       " 'short',\n",
       " 'theological',\n",
       " 'treatise',\n",
       " 'some',\n",
       " 'significance',\n",
       " 'seems',\n",
       " 'however',\n",
       " 'that',\n",
       " 'the',\n",
       " 'main',\n",
       " 'message',\n",
       " 'from',\n",
       " 'the',\n",
       " 'alleged',\n",
       " 'mother',\n",
       " 'god',\n",
       " 'was',\n",
       " 'pray',\n",
       " 'and',\n",
       " 'penance',\n",
       " 'for',\n",
       " 'the',\n",
       " 'conversion',\n",
       " 'the',\n",
       " 'world',\n",
       " 'take',\n",
       " 'drink',\n",
       " 'the',\n",
       " 'spring',\n",
       " 'nutshell',\n",
       " 'gods',\n",
       " 'are',\n",
       " 'beings',\n",
       " 'with',\n",
       " 'unnatural',\n",
       " 'powers',\n",
       " 'who',\n",
       " 'never',\n",
       " 'die',\n",
       " 'some',\n",
       " 'are',\n",
       " 'believed',\n",
       " 'the',\n",
       " 'controllers',\n",
       " 'creators',\n",
       " 'various',\n",
       " 'parts',\n",
       " 'nature',\n",
       " 'many',\n",
       " 'are',\n",
       " 'thought',\n",
       " 'require',\n",
       " 'worship',\n",
       " 'and',\n",
       " 'obedience',\n",
       " 'from',\n",
       " 'humans',\n",
       " 'these',\n",
       " 'gods',\n",
       " 'reward',\n",
       " 'punish',\n",
       " 'depending',\n",
       " 'whether',\n",
       " 'please',\n",
       " 'them',\n",
       " 'stories',\n",
       " 'gods',\n",
       " 'have',\n",
       " 'been',\n",
       " 'told',\n",
       " 'most',\n",
       " 'societies',\n",
       " 'that',\n",
       " 'know',\n",
       " 'going',\n",
       " 'back',\n",
       " 'least',\n",
       " '10000',\n",
       " 'years',\n",
       " 'gods',\n",
       " 'are',\n",
       " 'portrayed',\n",
       " 'beings',\n",
       " 'who',\n",
       " 'never',\n",
       " 'die',\n",
       " 'with',\n",
       " 'mighty',\n",
       " 'powers',\n",
       " 'able',\n",
       " 'make',\n",
       " 'nature',\n",
       " 'what',\n",
       " 'they',\n",
       " 'want',\n",
       " 'most',\n",
       " 'gods',\n",
       " 'are',\n",
       " 'pictured',\n",
       " 'being',\n",
       " 'born',\n",
       " 'and',\n",
       " 'having',\n",
       " 'parents',\n",
       " 'some',\n",
       " 'stories',\n",
       " 'show',\n",
       " 'gods',\n",
       " 'being',\n",
       " 'able',\n",
       " 'change',\n",
       " 'the',\n",
       " 'weather',\n",
       " 'and',\n",
       " 'cause',\n",
       " 'mighty',\n",
       " 'storms',\n",
       " 'floods',\n",
       " 'earthquakes',\n",
       " 'volcanic',\n",
       " 'eruptions',\n",
       " 'for',\n",
       " 'the',\n",
       " 'most',\n",
       " 'part',\n",
       " 'scientists',\n",
       " 'have',\n",
       " 'replaced',\n",
       " 'stories',\n",
       " 'about',\n",
       " 'gods',\n",
       " 'with',\n",
       " 'scientific',\n",
       " 'explanations',\n",
       " 'how',\n",
       " 'the',\n",
       " 'universe',\n",
       " 'was',\n",
       " 'formed',\n",
       " 'and',\n",
       " 'how',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'other',\n",
       " 'living',\n",
       " 'creatures',\n",
       " '2010',\n",
       " 'belief',\n",
       " 'omnipotent',\n",
       " 'omniscient',\n",
       " 'creator',\n",
       " 'the',\n",
       " 'world',\n",
       " 'does',\n",
       " 'not',\n",
       " 'itself',\n",
       " 'have',\n",
       " 'any',\n",
       " 'moral',\n",
       " 'still',\n",
       " 'you',\n",
       " 'decide',\n",
       " 'whether',\n",
       " 'right',\n",
       " 'obey',\n",
       " 'his',\n",
       " 'human',\n",
       " 'beings',\n",
       " 'are',\n",
       " 'anything',\n",
       " 'special',\n",
       " 'are',\n",
       " 'the',\n",
       " 'creatures',\n",
       " 'that',\n",
       " 'must',\n",
       " 'ponder',\n",
       " 'and',\n",
       " 'talk',\n",
       " 'jay',\n",
       " 'jay',\n",
       " '1997',\n",
       " 'essay',\n",
       " 'innocently',\n",
       " 'lack',\n",
       " 'conflict',\n",
       " 'between',\n",
       " 'science',\n",
       " 'and',\n",
       " 'religion',\n",
       " 'arises',\n",
       " 'from',\n",
       " 'lack',\n",
       " 'overlap',\n",
       " 'between',\n",
       " 'their',\n",
       " 'respective',\n",
       " 'domains',\n",
       " 'professional',\n",
       " 'the',\n",
       " 'empirical',\n",
       " 'constitution',\n",
       " 'the',\n",
       " 'universe',\n",
       " 'and',\n",
       " 'religion',\n",
       " 'the',\n",
       " 'search',\n",
       " 'for',\n",
       " 'proper',\n",
       " 'ethical',\n",
       " 'values',\n",
       " 'and',\n",
       " 'the',\n",
       " 'spiritual',\n",
       " 'meaning',\n",
       " 'our',\n",
       " 'lives',\n",
       " 'the',\n",
       " 'attainment',\n",
       " 'wisdom',\n",
       " 'full',\n",
       " 'life',\n",
       " 'requires',\n",
       " 'extensive',\n",
       " 'attention',\n",
       " 'both',\n",
       " 'great',\n",
       " 'book',\n",
       " 'tells',\n",
       " 'that',\n",
       " 'the',\n",
       " 'truth',\n",
       " 'can',\n",
       " 'make',\n",
       " 'free',\n",
       " 'and',\n",
       " 'that',\n",
       " 'will',\n",
       " 'live',\n",
       " 'optimal',\n",
       " 'harmony',\n",
       " 'with',\n",
       " 'our',\n",
       " 'fellows',\n",
       " 'when',\n",
       " 'learn',\n",
       " 'justly',\n",
       " 'love',\n",
       " 'mercy',\n",
       " 'and',\n",
       " 'walk',\n",
       " 'all',\n",
       " 'hell',\n",
       " 'broke',\n",
       " 'jay',\n",
       " '1997',\n",
       " 'essay',\n",
       " 'innocently',\n",
       " 'lack',\n",
       " 'conflict',\n",
       " 'between',\n",
       " 'science',\n",
       " 'and',\n",
       " 'religion',\n",
       " 'arises',\n",
       " 'from',\n",
       " 'lack',\n",
       " 'overlap',\n",
       " 'between',\n",
       " 'their',\n",
       " 'respective',\n",
       " 'domains',\n",
       " 'professional',\n",
       " 'the',\n",
       " 'empirical',\n",
       " 'constitution',\n",
       " 'the',\n",
       " 'universe',\n",
       " 'and',\n",
       " 'religion',\n",
       " 'the',\n",
       " 'search',\n",
       " 'for',\n",
       " 'proper',\n",
       " 'ethical',\n",
       " 'values',\n",
       " 'and',\n",
       " 'the',\n",
       " 'spiritual',\n",
       " 'meaning',\n",
       " 'our',\n",
       " 'lives',\n",
       " 'the',\n",
       " 'attainment',\n",
       " 'wisdom',\n",
       " 'full',\n",
       " 'life',\n",
       " 'requires',\n",
       " 'extensive',\n",
       " 'attention',\n",
       " 'both',\n",
       " 'great',\n",
       " 'book',\n",
       " 'tells',\n",
       " 'that',\n",
       " 'the',\n",
       " 'truth',\n",
       " 'can',\n",
       " 'make',\n",
       " 'free',\n",
       " 'and',\n",
       " 'that',\n",
       " 'will',\n",
       " 'live',\n",
       " 'optimal',\n",
       " 'harmony',\n",
       " 'with',\n",
       " 'our',\n",
       " 'fellows',\n",
       " 'when',\n",
       " 'learn',\n",
       " 'justly',\n",
       " 'love',\n",
       " 'mercy',\n",
       " 'and',\n",
       " 'walk',\n",
       " 'all',\n",
       " 'hell',\n",
       " 'broke',\n",
       " 'jay',\n",
       " '1997',\n",
       " 'essay',\n",
       " 'innocently',\n",
       " 'wrote',\n",
       " 'the',\n",
       " 'lack',\n",
       " 'conflict',\n",
       " 'between',\n",
       " 'science',\n",
       " 'and',\n",
       " 'religion',\n",
       " 'arises',\n",
       " 'from',\n",
       " 'lack',\n",
       " 'overlap',\n",
       " 'between',\n",
       " 'their',\n",
       " 'respective',\n",
       " 'domains',\n",
       " 'professional',\n",
       " 'the',\n",
       " 'empirical',\n",
       " 'constitution',\n",
       " 'the',\n",
       " 'universe',\n",
       " 'and',\n",
       " 'religion',\n",
       " 'the',\n",
       " 'search',\n",
       " 'for',\n",
       " 'proper',\n",
       " 'ethical',\n",
       " 'values',\n",
       " 'and',\n",
       " 'the',\n",
       " 'spiritual',\n",
       " 'meaning',\n",
       " 'our',\n",
       " 'lives',\n",
       " 'the',\n",
       " 'attainment',\n",
       " 'wisdom',\n",
       " 'full',\n",
       " 'life',\n",
       " 'requires',\n",
       " 'extensive',\n",
       " 'attention',\n",
       " 'both',\n",
       " 'great',\n",
       " 'book',\n",
       " 'tells',\n",
       " 'that',\n",
       " 'the',\n",
       " 'truth',\n",
       " 'can',\n",
       " 'make',\n",
       " 'free',\n",
       " 'and',\n",
       " 'that',\n",
       " 'will',\n",
       " 'live',\n",
       " 'optimal',\n",
       " 'harmony',\n",
       " 'with',\n",
       " 'our',\n",
       " 'fellows',\n",
       " 'when',\n",
       " 'learn',\n",
       " 'justly',\n",
       " 'love',\n",
       " 'mercy',\n",
       " 'and',\n",
       " 'walk',\n",
       " 'humbly',\n",
       " 'then',\n",
       " 'all',\n",
       " 'hell',\n",
       " 'broke',\n",
       " 'books',\n",
       " 'ordering',\n",
       " 'information',\n",
       " 'print',\n",
       " 'versions',\n",
       " 'available']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_page = Webpage(url)\n",
    "fname_text = new_page.hash+'.text'\n",
    "fname_links = new_page.hash+'.links'\n",
    "if os.path.isfile(path+fname_text): \n",
    "    new_page.load_text(path, fname_text)\n",
    "    print(\"Loading Text\")\n",
    "else:\n",
    "    new_page.get_page_html()\n",
    "    new_page.get_html_text(tags=[\"p\",\"h1\",\"h2\",\"h3\",\"span\"])\n",
    "    new_page.clean_html_text(500, ignore=english_words[:50])\n",
    "    new_page.save_text(path, fname_text)\n",
    "\n",
    "if os.path.isfile(path+fname_links): \n",
    "    new_page.load_links(path, fname_links)\n",
    "    print(\"Loading Links\")\n",
    "else:\n",
    "    new_page.get_page_html()\n",
    "    new_page.get_html_anchors()\n",
    "    new_page.save_links(path, fname_links)\n",
    "new_page.k_common_words(k=5,ignore=english_words[:50])\n",
    "new_page.cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_page_all(url, k, max_words, ignore_words, path = None):\n",
    "    page = Webpage(url)\n",
    "    fname_text = page.hash+'.text'\n",
    "    fname_links = page.hash+'.links'\n",
    "    if path == None:\n",
    "        page.get_page_html()\n",
    "        page.get_html_text(tags=[\"p\",\"h1\",\"h2\",\"h3\",\"span\"])\n",
    "        page.get_html_anchors()\n",
    "        page.clean_html_text(max_words, ignore=english_words[:50])\n",
    "    else:\n",
    "        if os.path.isfile(path+fname_text): \n",
    "            page.load_text(path, fname_text)\n",
    "        else:\n",
    "            page.get_page_html()\n",
    "            page.get_html_text(tags=[\"p\",\"h1\",\"h2\",\"h3\",\"span\"])\n",
    "            page.clean_html_text(max_words, ignore=english_words[:50])\n",
    "            page.save_text(path, fname_text)\n",
    "\n",
    "        if os.path.isfile(path+fname_links): \n",
    "            page.load_links(path, fname_links)\n",
    "        else:\n",
    "            if page.html == \"\": page.get_page_html()\n",
    "            page.get_html_anchors()\n",
    "            page.save_links(path, fname_links)\n",
    "\n",
    "    if page.cleaned_text is not None:\n",
    "        page.k_common_words(k=k, ignore=ignore_words)\n",
    "    return page\n",
    "\n",
    "def get_all_links(url, dict, k, min_words=20, max_words=500, ignore_words=[], ignore_filenames=[\".mp3\",\".jpg\",\".png\"], max_links=\"\", path=None):\n",
    "    page = get_page_all(url, k, max_words, ignore_words, path)\n",
    "    if page.cleaned_text is not []:\n",
    "        dict[url] = [page.cleaned_text, page.most_common_words]\n",
    "        print(url,\"Contains\",len(page.links),\"Links\")\n",
    "        if max_links == \"\" or max_links > len(page.links): max_links=len(page.links)\n",
    "        \n",
    "        for link in page.links[:max_links]:\n",
    "            if all(x not in link for x in ignore_filenames):\n",
    "                try:\n",
    "                    page = get_page_all(link, k, max_words, ignore_words, path)\n",
    "                    if page.cleaned_text is not []:\n",
    "                        if len(page.cleaned_text) < min_words: continue\n",
    "                        dict[link] = [page.cleaned_text, page.most_common_words]\n",
    "                except:\n",
    "                    pass\n",
    "    else:\n",
    "        print(url,\"returned None, Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30 # words\n",
    "min_words = 50\n",
    "max_words = 450\n",
    "max_links = 100\n",
    "ignore_words = ['the', 'of', 'to', 'and', 'a', 'in', 'it', 'that', 'for', 'on'] #english_words[:20]\n",
    "ignore_filenames = [\".mp3\",\".jpg\",\".png\",\".mp4\",\".jfif\",\"facebook.com\",\"twitter.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#d_pse = {}\n",
    "#get_all_links(pseudo_sources[2], d_pse, k, min_text_len, ignore_words, ignore_filenames)\n",
    "#d_pse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.ageofautism.com/ Contains 705 Links\n",
      "http://www.naturalnews.com Contains 323 Links\n",
      "https://foodbabe.com/starthere/ Contains 124 Links\n",
      "http://www.chopra.com Contains 100 Links\n",
      "https://www.mercola.com/ Contains 125 Links\n",
      "https://www.history.com/ Contains 84 Links\n",
      "https://doctoroz.com/ Contains 28 Links\n"
     ]
    }
   ],
   "source": [
    "d_pse = {}\n",
    "d_sci = {}\n",
    "path = os.getcwd()+'/data/'\n",
    "if os.path.isdir(path) is False: os.mkdir(path)\n",
    "path_pse = path+'pseudoscience/'\n",
    "path_sci = path+'science/'\n",
    "if os.path.isdir(path_pse) is False: os.mkdir(path_pse)\n",
    "if os.path.isdir(path_sci) is False: os.mkdir(path_sci)\n",
    "\n",
    "for source in pseudo_sources:\n",
    "    get_all_links(source, d_pse, k, min_words, max_words, ignore_words, ignore_filenames, max_links, path_pse)\n",
    "for source in science_sources:\n",
    "    get_all_links(source, d_sci, k, min_words, max_words, ignore_words, ignore_filenames, max_links, path_sci)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Pseudoscience 30 Most Common Words ####\n",
      " [('health', 2767), ('our', 2056), ('2022', 2031), ('vaccine', 1420), ('food', 1282), ('autism', 1175), ('posted', 1158), ('its', 1091), ('comment', 1090), ('information', 863), ('news', 831), ('comments', 726), ('new', 720), ('children', 714), ('after', 657), ('public', 646), ('email', 621), ('any', 582), ('twitter', 546), ('age', 522), ('also', 509), ('please', 494), ('natural', 492), ('free', 478), ('just', 462), ('vaccines', 456), ('climate', 451), ('years', 450), ('medical', 440), ('share', 437)] \n",
      "\n",
      "\n",
      "#### Science 30 Most Common Words ####\n",
      " [('health', 1678), ('medicine', 1468), ('science', 1288), ('our', 1251), ('2022', 1165), ('climate', 1137), ('14106144685', 930), ('services', 905), ('menu', 895), ('711', 868), ('human', 818), ('news', 748), ('new', 731), ('department', 719), ('information', 714), ('patients', 684), ('care', 665), ('its', 628), ('research', 625), ('access', 623), ('used', 567), ('share', 565), ('johns', 527), ('change', 497), ('marketing', 492), ('rights', 488), ('civil', 483), ('data', 472), ('also', 469), ('global', 462)] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_pse = Counter()\n",
    "count_sci = Counter()\n",
    "for link in d_pse:\n",
    "    count_pse+=Counter(dict(d_pse[link][1]))\n",
    "for link in d_sci:\n",
    "    count_sci+=Counter(dict(d_sci[link][1]))\n",
    "\n",
    "print(\"#### Pseudoscience\",k,\"Most Common Words ####\\n\",count_pse.most_common(k),\"\\n\\n\")\n",
    "print(\"#### Science\",k,\"Most Common Words ####\\n\",count_sci.most_common(k),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cookiedatabase.org/tcf/purposes/\n"
     ]
    }
   ],
   "source": [
    "for link in d_pse:\n",
    "    if link in d_sci.keys():\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>common_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://www.ageofautism.com/</th>\n",
       "      <td>during spring kvetched twitter slapped sensitive content label our tweets included link photo before musk stepped see imagine twitter became open again hope twitter exposes censorship many faced especially crackdown mean want see overtly controversial despicable content allowed who defines pornography right know see defender twitter ditches misinformation policy plans reveal internal files free speech suppression effective twitter longer enforcing misleading information policy company adding will soon reveal internal files free speech suppression which could shed light twitters past action...</td>\n",
       "      <td>twitter autism 2022 vaccine age deaths content link policy information its also sudden between posted comments epidemic gender issues take misinformation accounts analysis health current affairs cause unknown 2021 young</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.ageofautism.com/</th>\n",
       "      <td>during spring kvetched twitter slapped sensitive content label our tweets included link photo before musk stepped see imagine twitter became open again hope twitter exposes censorship many faced especially crackdown mean want see overtly controversial despicable content allowed who defines pornography right know see defender twitter ditches misinformation policy plans reveal internal files free speech suppression effective twitter longer enforcing misleading information policy company adding will soon reveal internal files free speech suppression which could shed light twitters past action...</td>\n",
       "      <td>twitter autism 2022 vaccine age deaths content link policy information its also sudden between posted comments epidemic gender issues take misinformation accounts analysis health current affairs cause unknown 2021 young</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.ageofautism.com/exclusives.html</th>\n",
       "      <td>editorials series here come elaborate fraud series deer special report epidemiological studies really tell note epidemiological studies here vaccines autism these studies represent most often cited papers scientists public health officials members media trying refute any evidence association between vaccinations autism serious methodological limitations design flaws conflicts interest problems related these studies these flaws been pointed government officials researchers medical review panels even authors studies themselves taken together limitations these studies make impossible conclude...</td>\n",
       "      <td>autism studies health series here public epidemiological vaccines officials limitations flaws study defense donate editorials elaborate fraud deer special report really tell note represent often cited papers scientists members media</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.ageofautism.com/science/</th>\n",
       "      <td>tom urged get their bivalent vaccine booster yesterday twitter kindly let him followers know about week class starting week through countermeasures injury compensation program program absolves corporations whose products harm during pandemic take course both live recorded version national vaccine injury compensation program countermeasures injury compensation program used emergency authorized course students will learn structure function defects programs created congress award compensation adults children potentially actually harmed killed vaccines will review variety important topics revi...</td>\n",
       "      <td>2020 autism health high compensation information consequence disease vaccine injury program march course available age public children parents posted government increase letter 2019 countermeasures during national new 2022 science comments</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.ageofautism.com/a-welcome-from-dan-olmste.html</th>\n",
       "      <td>welcome age autism daily web newspaper autism epidemic donate please either donate button right sidebar secure accepts credit cards send check autism age box 110546 06611 donations tax deductible our nonprofit 471831987 thank published give voice those who believe autism environmentally induced illness treatable children recover most part major media united states interested point view wont investigate causes possible biomedical treatments autism independently listen most important people parents many whom witnessed autistic regression medical illness after vaccinations those things more b...</td>\n",
       "      <td>autism comments donate health age our those epidemic believe illness medical defense public welcome daily web newspaper please either button right sidebar secure accepts credit cards send check box 110546</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               text  \\\n",
       "http://www.ageofautism.com/                                 during spring kvetched twitter slapped sensitive content label our tweets included link photo before musk stepped see imagine twitter became open again hope twitter exposes censorship many faced especially crackdown mean want see overtly controversial despicable content allowed who defines pornography right know see defender twitter ditches misinformation policy plans reveal internal files free speech suppression effective twitter longer enforcing misleading information policy company adding will soon reveal internal files free speech suppression which could shed light twitters past action...   \n",
       "https://www.ageofautism.com/                                during spring kvetched twitter slapped sensitive content label our tweets included link photo before musk stepped see imagine twitter became open again hope twitter exposes censorship many faced especially crackdown mean want see overtly controversial despicable content allowed who defines pornography right know see defender twitter ditches misinformation policy plans reveal internal files free speech suppression effective twitter longer enforcing misleading information policy company adding will soon reveal internal files free speech suppression which could shed light twitters past action...   \n",
       "https://www.ageofautism.com/exclusives.html                 editorials series here come elaborate fraud series deer special report epidemiological studies really tell note epidemiological studies here vaccines autism these studies represent most often cited papers scientists public health officials members media trying refute any evidence association between vaccinations autism serious methodological limitations design flaws conflicts interest problems related these studies these flaws been pointed government officials researchers medical review panels even authors studies themselves taken together limitations these studies make impossible conclude...   \n",
       "https://www.ageofautism.com/science/                        tom urged get their bivalent vaccine booster yesterday twitter kindly let him followers know about week class starting week through countermeasures injury compensation program program absolves corporations whose products harm during pandemic take course both live recorded version national vaccine injury compensation program countermeasures injury compensation program used emergency authorized course students will learn structure function defects programs created congress award compensation adults children potentially actually harmed killed vaccines will review variety important topics revi...   \n",
       "https://www.ageofautism.com/a-welcome-from-dan-olmste.html  welcome age autism daily web newspaper autism epidemic donate please either donate button right sidebar secure accepts credit cards send check autism age box 110546 06611 donations tax deductible our nonprofit 471831987 thank published give voice those who believe autism environmentally induced illness treatable children recover most part major media united states interested point view wont investigate causes possible biomedical treatments autism independently listen most important people parents many whom witnessed autistic regression medical illness after vaccinations those things more b...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                               common_words  \\\n",
       "http://www.ageofautism.com/                                                     twitter autism 2022 vaccine age deaths content link policy information its also sudden between posted comments epidemic gender issues take misinformation accounts analysis health current affairs cause unknown 2021 young   \n",
       "https://www.ageofautism.com/                                                    twitter autism 2022 vaccine age deaths content link policy information its also sudden between posted comments epidemic gender issues take misinformation accounts analysis health current affairs cause unknown 2021 young   \n",
       "https://www.ageofautism.com/exclusives.html                        autism studies health series here public epidemiological vaccines officials limitations flaws study defense donate editorials elaborate fraud deer special report really tell note represent often cited papers scientists members media   \n",
       "https://www.ageofautism.com/science/                        2020 autism health high compensation information consequence disease vaccine injury program march course available age public children parents posted government increase letter 2019 countermeasures during national new 2022 science comments   \n",
       "https://www.ageofautism.com/a-welcome-from-dan-olmste.html                                     autism comments donate health age our those epidemic believe illness medical defense public welcome daily web newspaper please either button right sidebar secure accepts credit cards send check box 110546   \n",
       "\n",
       "                                                                    label  \n",
       "http://www.ageofautism.com/                                 pseudoscience  \n",
       "https://www.ageofautism.com/                                pseudoscience  \n",
       "https://www.ageofautism.com/exclusives.html                 pseudoscience  \n",
       "https://www.ageofautism.com/science/                        pseudoscience  \n",
       "https://www.ageofautism.com/a-welcome-from-dan-olmste.html  pseudoscience  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_all = {}\n",
    "for link in d_pse:\n",
    "    text = d_pse[link][0]\n",
    "    if len(text) > max_words: text = text[:max_words]\n",
    "    common_words = ' '.join([count[0] for count in d_pse[link][1]])\n",
    "    if link not in d_all:\n",
    "        d_all[link] = [' '.join(text), common_words, 'pseudoscience']\n",
    "\n",
    "for link in d_sci:\n",
    "    text = d_sci[link][0]\n",
    "    if len(text) > max_words: text = text[:max_words]\n",
    "    common_words = ' '.join([count[0] for count in d_sci[link][1]])\n",
    "    if link not in d_all:\n",
    "        d_all[link] = [' '.join(text), common_words, 'science']\n",
    "\n",
    "df = pd.DataFrame.from_dict(d_all, orient='index', columns=['text', 'common_words', 'label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos och system man den evolution may also study more restricted problem ancestry compare model common ancestry two species unique origin model according which species founded single couple carter 2014 carter 2018 2019 och xxunk den med den man par man och och den art 7 xxrep 4 0 sedan 1 xxrep 4 0 man men den variation till 5 xxrep 5 0 scenario 6 xxrep 3 0 till 1 xxrep 4 0 sedan man man genetic modeling human history part comparison common descent unique origin approaches och genetic modeling human history part unique origin algorithm 2016 med med till discovery institute human origin possible 2019 man sin man par 500 xxrep 3 0 sedan med men 100 xxrep 3 0 discovery institute den den genetic modeling human history part comparison common descent unique origin approaches 2016 man argue unique origin model where humanity arose single couple created diversity seems</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos heartland institute proud make available parry memorial library nations best libraries limited government nearly 2 xxrep 4 0 library located heartland xxunk north heights library online database also book wish list amazon check our list then check collection books may willing donate see book donation guidelines below parry library holds nearly 2 xxrep 4 0 books history economics education environment issues health care policy law philosophy topics collection will special interest students scholars studying economics political science elected officials members their xxunk concerned citizens watch grand opening presentations below read 2016 parry library accepted membership rails reaching across library system rails serves approximately 1300 academic public school special library agencies northern collection offers books following topic areas constantly updated collection available online online searchable database library open public 900 500 through admission fee visitors asked call 3123774 xxrep 3 0 make appointment library time lending library patrons study space</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos continuing browse our site agree our cookies revised privacy policy terms service agree group nations including declaring digital vaccine read more future headed into world where everyone will augmented technology lower class controlled read more evidence shows unprocessed red meat health risk however vilification natural raw meat products raises potential during victory gardens produced produce many now rediscovering many benefits gardening offers fresh inventory control plan just about everything earth land water minerals plants animals food energy genetically engineered foods grain crops like wheat oats barley common sources exposure toxic chemical linked even small amount just these vitamins risk dying even most severe cases drops like rock fact before musk bought twitter social media giant suspended 11 xxrep 3 0 accounts removed 1 xxrep 5 0 pieces content violating its help medical records more than 1 xxrep 3 0 health care workers interacting directly patients researchers concluded world experiencing</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = TextDataLoaders.from_df(df, bs=8, text_col='text', label_col='label')\n",
    "dls.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.580649</td>\n",
       "      <td>0.391814</td>\n",
       "      <td>0.846690</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.475741</td>\n",
       "      <td>0.460909</td>\n",
       "      <td>0.839721</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.452111</td>\n",
       "      <td>0.252557</td>\n",
       "      <td>0.909408</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.359433</td>\n",
       "      <td>0.267908</td>\n",
       "      <td>0.885017</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.282798</td>\n",
       "      <td>0.250456</td>\n",
       "      <td>0.909408</td>\n",
       "      <td>01:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "learn.fine_tune(4, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Device 0 | NVIDIA GeForce RTX 3050 Ti Laptop GPU | Allocated: 0.4 GB | Cached: 2.2 GB\n"
     ]
    }
   ],
   "source": [
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos 365 369 373 xxunk xxunk xxunk xxrep 10 0 xxunk xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep 5 0 xxrep</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos 2022 before musk bought twitter social media giant suspended 11 xxrep 3 0 accounts removed 1 xxrep 5 0 pieces content violating its misinformation policy between 2020 2022 alone now musk owns has made 2022 help medical records more than 1 xxrep 3 0 health care workers interacting directly patients researchers concluded medical masks provide little benefit over respirators specifically researchers 2022 world experiencing strongest decline birth rates over 100 years its unprecedented figure yet except laypersons like educators independent researchers statisticians true health official making comments 2022 more than 230 million having received least two shots first time since shots came more vaccinated than persons dying infection just those eligible new boosters 2022 effort discredit thoroughly sourced article risks benefits treatment goes great lengths literally prove pill does indeed prevent deaths 2022 now he s finally under deposition role pandemic censorship went national institute allergy infectious diseases director</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos guide legal matters 2017 federal court ordered aka belle pay engaging unconscionable conduct relating sale her book app whole pantry described below court found falsely stated been diagnosed brain cancer given only four months live after rejecting standard treatment cured herself natural methods portion proceeds book described her methods would donated charities court also ordered pay costs consumer affairs which initiated case against her 2018 reported paid money consumer affairs seeking court order enable xxunk her xxunk court consumer law xxunk 247 xxunk first respondent road xxunk ltd xxunk xxunk xxunk xxunk second respondent misleading deceptive conduct consumer law consumer law engaged conduct trade commerce which misleading deceptive likely mislead deceive contrary consumer law approximately 2013 made claims connection development promotion sale whole pantry app whole pantry book second respondent engaged conduct trade commerce which misleading deceptive likely mislead deceive contrary consumer law consumer law about 2013 made claims</td>\n",
       "      <td>science</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos children today previous generations proof news coverage see every day site shows what s happening schools around increasingly disabled chronically ill education system has accommodate them long associated autism like sensory issues repetitive behaviors lack social skills now problems affecting mainstream students blame predictably placed bad parenting otherwise known trauma home addressing mental health needs important academics modern educators unrecognized disaster here about children who ca nt learn behave like children always been expected childhood has become chilling portent future mankind join our mailing list never miss update thanks message sent medical center host sensory friendly holiday event schools unable meet diverse levels debt new autism school open sped workers parents want meeting xxunk xxunk school marks 11th anniversary sensory room opening parents take diagnose dump treatment autism cape sensory friendly safe place watch parade lights special high school add autism places increasing demand new suspensions disproportionately affect students</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos can not manufacture any products come oil which supporting eight billion now planet conference held attracted global elites more than four hundred private jets attendees recognize climate change occurring like has four billion years seems most lacked basic energy literacy starts knowledge renewable energy only intermittent electricity generated unreliable breezes sunshine wind turbines solar panels can not manufacture anything billion planet xxunk xxunk facts like wind turbines solar panels can not manufacture any oil derivatives basis thousands products foundation societies economies around world fossil fuel products reasons world xxunk billion less than two hundred years much world leaders wish rid world emissions fossil fuels world has yet identify replacement oil derivatives basis more than 6 xxrep 3 0 products fuels our various transportation infrastructures cop attendees should also know crude oil useless unless manufactured into something xxunk like fuels xxunk transportation infrastructures ships jets derivatives make more than 6</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos exchange consists seven posts comment reply plus post translation mostly google translate original 2020 posts comments kindly corrected translation welcome comment xxunk thumb keep mind posts these xxunk comments written two years ago thus can not take more recent comments here into account hope publish sections seven consecutive chose day week without remembering its connection ancient days readers these authors invited contribute further comments our comment system weeks post starts introduction followed first posts comment followed xxunk two years ago professors informatics respectively published article titled using statistical methods model molecular machines systems journal theoretical biology regular scientific intelligent design community publication seen breakthrough close reading paper however convincing seven blog posts blog run staff institute biology environmental science university therefore detailed critique four specific points also paper could published article title using statistical methods model well xxunk article clearly statistical methods been used demonstrate cell however calculations presented</td>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos food babe whenever enter conventional grocery store get heart xxunk might think kidding about body gets heated face starts xxunk end saying about times before leave store think now would emotions under control think why passionate about fixing food system know too because keep emailing commenting xxunk xxunk things seeing ca nt thank enough investigation been asking here new food babe definitely want read before ever deli counter again received emails comments social media expressing been led believe boars head deli meat than brands wanted know really better just got really good marketing compromise elsewhere sure makes sound like add cheap additives their deli meats really emails would love eat boars head deli meats xxunk disease seemed leading company deli meats called company directly ingredient lists favorite deli meats discovered caramel coloring which know good lot their offerings could food babe investigate further start campaign get them take offending</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos google has declared war independent media has begun blocking emails getting our readers recommend free uncensored email receiving service free encrypted email send receive service okay continue address 2022 huff tags big tech brainwashed computing conspiracy deception deep state musk glitch globalist narrative information technology mind control propaganda psychological operations social media tech giants technocrats article may contain statements reflect opinion author natural news portion twitters user base propagandists pushing psychological operations ops according musk new head twitter empire amount pro ops twitter ridiculous musk tweeted adding joke least new verified will pay privilege related 2014 musk warned artificial intelligence has potential completely wipe humanity later asked explain nature these ops musk clarified most them appear rather than professional adding its mostly basic simply put operation usually led state actors such military aims disseminate propaganda purpose psychological warfare manipulation targets thoughts beliefs unclear specifically musk referring pro ops only</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sources = {'https://infowarslife.com/':'pseudoscience',\n",
    "'https://www.bbc.com/news/':'science',\n",
    "'https://www.dailymail.co.uk/':'pseudoscience',\n",
    "'https://www.si.edu/explore/science':'science',\n",
    "'https://www.foxnews.com/opinion':'pseudoscience',\n",
    "'https://www.disclose.tv/':'pseudoscience',\n",
    "'https://www.snopes.com/top/':'science',\n",
    "'https://www.theskepticsguide.org/about':'science',\n",
    "'https://www.cdc.gov/':'science',\n",
    "'https://www.motherjones.com/':'pseudoscience',\n",
    "'https://www.huffpost.com/':'pseudoscience',\n",
    "'https://arstechnica.com/':'science',\n",
    "'https://nationalreport.net/':'pseudoscience',\n",
    "'https://newspunch.com/':'pseudoscience'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>https://infowarslife.com/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.701568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.bbc.com/news/</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.834534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.dailymail.co.uk/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.986453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.si.edu/explore/science</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.801990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.foxnews.com/opinion</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.958947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.disclose.tv/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.981773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.snopes.com/top/</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.983805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.theskepticsguide.org/about</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.800767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.cdc.gov/</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.985176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.motherjones.com/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>science</td>\n",
       "      <td>0.903082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.huffpost.com/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.731096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://arstechnica.com/</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.889102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://nationalreport.net/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.754702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://newspunch.com/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.978445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               actual     prediction  \\\n",
       "https://infowarslife.com/               pseudoscience  pseudoscience   \n",
       "https://www.bbc.com/news/                     science        science   \n",
       "https://www.dailymail.co.uk/            pseudoscience  pseudoscience   \n",
       "https://www.si.edu/explore/science            science        science   \n",
       "https://www.foxnews.com/opinion         pseudoscience  pseudoscience   \n",
       "https://www.disclose.tv/                pseudoscience  pseudoscience   \n",
       "https://www.snopes.com/top/                   science        science   \n",
       "https://www.theskepticsguide.org/about        science        science   \n",
       "https://www.cdc.gov/                          science        science   \n",
       "https://www.motherjones.com/            pseudoscience        science   \n",
       "https://www.huffpost.com/               pseudoscience  pseudoscience   \n",
       "https://arstechnica.com/                      science        science   \n",
       "https://nationalreport.net/             pseudoscience  pseudoscience   \n",
       "https://newspunch.com/                  pseudoscience  pseudoscience   \n",
       "\n",
       "                                        probability  \n",
       "https://infowarslife.com/                  0.701568  \n",
       "https://www.bbc.com/news/                  0.834534  \n",
       "https://www.dailymail.co.uk/               0.986453  \n",
       "https://www.si.edu/explore/science         0.801990  \n",
       "https://www.foxnews.com/opinion            0.958947  \n",
       "https://www.disclose.tv/                   0.981773  \n",
       "https://www.snopes.com/top/                0.983805  \n",
       "https://www.theskepticsguide.org/about     0.800767  \n",
       "https://www.cdc.gov/                       0.985176  \n",
       "https://www.motherjones.com/               0.903082  \n",
       "https://www.huffpost.com/                  0.731096  \n",
       "https://arstechnica.com/                   0.889102  \n",
       "https://nationalreport.net/                0.754702  \n",
       "https://newspunch.com/                     0.978445  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_pred = {}\n",
    "\n",
    "for source in test_sources:\n",
    "    page = get_page_all(source, k, max_words, ignore_words)\n",
    "    length = len(page.cleaned_text)\n",
    "    if  length < min_words:\n",
    "        print(\"ERROR:\",source,length,\"words\")\n",
    "    else:\n",
    "        common_words = ' '.join([count[0] for count in page.most_common_words])\n",
    "        text = ' '.join(page.cleaned_text)\n",
    "        with learn.no_bar(), learn.no_logging():\n",
    "            prediction = learn.predict(text)\n",
    "        if prediction[0] == \"science\":\n",
    "            p = prediction[2][1].item()\n",
    "        else:\n",
    "            p = prediction[2][0].item()\n",
    "        d_pred[source] = [test_sources[source], prediction[0], p]\n",
    "\n",
    "df = pd.DataFrame.from_dict(d_pred, orient='index', columns=['actual', 'prediction', 'probability'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.save('2022.11.29 Model v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = load_learner('models/2022.11.28 Model.pth', cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pseudometer",
   "language": "python",
   "name": "pseudometer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
