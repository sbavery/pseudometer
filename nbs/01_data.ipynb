{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> Web scraping and tools for data collection and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sbavery/pseudometer/blob/main/nbs/01_data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "## Google Colab / Enchant Library Install for Dictionaries\n",
    "#!apt update\n",
    "#!apt install enchant-2 --fix-missing\n",
    "#!apt install -qq enchant-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import enchant\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "from fastai.text.all import *\n",
    "import hashlib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "## Utility Function to Check GPU Status\n",
    "def check_gpu():\n",
    "    print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    if num_devices > 0:\n",
    "        for device in range(0,num_devices):\n",
    "            print(\"Device\", device, \"|\", torch.cuda.get_device_name(device), \n",
    "            \"| Allocated:\", round(torch.cuda.memory_allocated(device)/1024**3,1), \"GB\",\n",
    "            \"| Cached:\", round(torch.cuda.memory_reserved(device)/1024**3,1), \"GB\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Device 0 | NVIDIA GeForce GTX 1050 Ti | Allocated: 0.4 GB | Cached: 0.6 GB\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "check_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Webpage:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.hash = self.get_hash_str()\n",
    "        self.requested = False\n",
    "        self.page_text = \"\"\n",
    "        self.html = \"\"\n",
    "        self.links = []\n",
    "        self.text = []\n",
    "        self.cleaned_text = []\n",
    "        self.most_common_words = []\n",
    "    \n",
    "    def get_page(self, headers, min_size, max_size):\n",
    "        r = requests.get(self.url, stream=True, headers=headers)\n",
    "        content_length = int(r.headers.get('Content-Length', 0))\n",
    "        data = []\n",
    "        length = 0\n",
    "\n",
    "        if content_length > max_size:\n",
    "            return None\n",
    "\n",
    "        for chunk in r.iter_content(1024):\n",
    "            data.append(chunk)\n",
    "            length += len(chunk)\n",
    "            if length > max_size:\n",
    "                return None\n",
    "        r._content = b''.join(data)\n",
    "        if len(r.text) < min_size: return None\n",
    "        return r.text\n",
    "\n",
    "    def get_page_html(self, min_size=1000, max_size=2000000):\n",
    "        user_agents = [ \n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36', \n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36', \n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36', \n",
    "            'Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148', \n",
    "            'Mozilla/5.0 (Linux; Android 11; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.72 Mobile Safari/537.36' \n",
    "        ] \n",
    "        user_agent = random.choice(user_agents) \n",
    "        headers = {'User-Agent': user_agent} \n",
    "        self.page_text = self.get_page(headers, min_size, max_size)\n",
    "        self.html = BeautifulSoup(self.page_text, \"html.parser\")\n",
    "        self.requested = True\n",
    "\n",
    "    def get_hash_str(self, inp=\"\"):\n",
    "        return hashlib.sha3_256((self.url+inp).encode()).hexdigest()\n",
    "\n",
    "    def get_html_anchors(self, keyword=\"http\"):\n",
    "        for anchor in self.html.findAll('a'):\n",
    "            link = anchor.get('href')\n",
    "            if link == None or link == \"\":\n",
    "                continue\n",
    "            if keyword in link:\n",
    "                self.links.append(link)\n",
    "                \n",
    "    def get_html_text(self, tags=[\"p\"]):\n",
    "        for tag in tags:\n",
    "            for p in self.html.findAll(tag):\n",
    "                p_text = p.getText().strip()\n",
    "                if p_text == None or p_text == '':\n",
    "                    continue\n",
    "                self.text.append(p_text)\n",
    "\n",
    "    def clean_html_text(self, max_words, enchant_dict=\"en_US\", ignore=[], rx=\"[^a-zA-Z ]+\", min_word_len=2):\n",
    "        all_text = ' '.join(self.text).lower()\n",
    "        regex_text = re.sub(rx,'',all_text).strip()\n",
    "        split = regex_text.split()\n",
    "        split = [word for word in split if word not in ignore]\n",
    "        if enchant_dict != \"\": d = enchant.Dict(enchant_dict)\n",
    "        for word in split:\n",
    "            if len(self.cleaned_text) >= max_words: break\n",
    "            if len(word) >= min_word_len:\n",
    "                if enchant_dict == \"\":\n",
    "                    self.cleaned_text.append(word)\n",
    "                elif d.check(word): \n",
    "                    self.cleaned_text.append(word)\n",
    "\n",
    "    def k_common_words(self, k=10, ignore=[]):\n",
    "        if self.cleaned_text == \"\":\n",
    "            text = self.text\n",
    "        else:\n",
    "            text = self.cleaned_text\n",
    "        all_text = ' '.join(text).lower()\n",
    "        split = all_text.split()\n",
    "        split_ignore = [word for word in split if word not in ignore]\n",
    "        counts = Counter(split_ignore)\n",
    "        k_most_common = counts.most_common(k)\n",
    "        self.most_common_words = k_most_common\n",
    "\n",
    "    def save_text(self, path, fname):\n",
    "        file = open(path+fname, 'wb')\n",
    "        pickle.dump(self.text, file)\n",
    "        file.close()\n",
    "\n",
    "    def load_text(self, path, fname):\n",
    "        file = open(path+fname, 'rb')\n",
    "        self.text = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    def save_links(self, path, fname):\n",
    "        file = open(path+fname, 'wb')\n",
    "        pickle.dump(self.links, file)\n",
    "        file.close()\n",
    "\n",
    "    def load_links(self, path, fname):\n",
    "        file = open(path+fname, 'rb')\n",
    "        self.links = pickle.load(file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 most common English words\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "url = \"https://gist.githubusercontent.com/deekayen/4148741/raw/98d35708fa344717d8eee15d11987de6c8e26d7d/1-1000.txt\"\n",
    "common_english = Webpage(url)\n",
    "common_english.get_page_html(min_size=1000)\n",
    "english_words = common_english.html.getText().lower()\n",
    "english_words = english_words.split('\\n')\n",
    "print(len(english_words),\"most common English words\")\n",
    "#english_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_sources = [\"http://www.ageofautism.com/\",\n",
    " \"http://www.naturalnews.com\", \n",
    " \"https://foodbabe.com/starthere/\",\n",
    " \"http://www.chopra.com\",\n",
    " \"https://www.mercola.com/\",\n",
    " \"https://www.history.com/\",\n",
    " \"https://doctoroz.com/\",\n",
    " \"https://www.disclose.tv/\",\n",
    " \"https://nationalreport.net/\",\n",
    " \"https://heartland.org/\",\n",
    " \"https://www.dailymail.co.uk/\",\n",
    " \"https://www.motherjones.com/\"]\n",
    "\n",
    "science_sources = [\"https://sciencebasedmedicine.org/\",\n",
    " \"https://www.hopkinsmedicine.org/gim/research/method/ebm.html\",\n",
    " \"https://www.bbc.com/news/science_and_environment\",\n",
    " \"https://www.nature.com/\",\n",
    " \"https://www.science.org/\",\n",
    " \"https://www.snopes.com/top/\",\n",
    " \"https://quackwatch.org/\",\n",
    " \"https://www.skepdic.com/\",\n",
    " \"http://scibabe.com/\",\n",
    " \"http://pandasthumb.org/\",\n",
    " \"https://skepticalscience.com/\",\n",
    " \"https://www.cdc.gov/\",\n",
    " \"https://apnews.com/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "url = science_sources[7]\n",
    "path = os.getcwd()+'/data/'\n",
    "if os.path.isdir(path) is False: os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_page = Webpage(url)\n",
    "test_page.get_page_html()\n",
    "test_page.get_html_text()\n",
    "test_page.get_html_anchors()\n",
    "test_page.clean_html_text(500, ignore=english_words[:50], rx=\"[^a-zA-Z ]+\")\n",
    "test_page.save_text(path, test_page.hash+'.text')\n",
    "test_page.save_links(path, test_page.hash+'.links')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Text\n",
      "Loading Links\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'skeptics dictionary features definitions arguments essays hundreds strange beliefs amusing deceptions dangerous delusions also features dozens entries logical fallacies cognitive biases perception sci'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_page = Webpage(url)\n",
    "fname_text = new_page.hash+'.text'\n",
    "fname_links = new_page.hash+'.links'\n",
    "if os.path.isfile(path+fname_text): \n",
    "    new_page.load_text(path, fname_text)\n",
    "    print(\"Loading Text\")\n",
    "else:\n",
    "    new_page.get_page_html()\n",
    "    new_page.get_html_text(tags=[\"p\",\"h1\",\"h2\",\"h3\",\"span\"])\n",
    "    new_page.save_text(path, fname_text)\n",
    "\n",
    "if os.path.isfile(path+fname_links): \n",
    "    new_page.load_links(path, fname_links)\n",
    "    print(\"Loading Links\")\n",
    "else:\n",
    "    new_page.get_page_html()\n",
    "    new_page.get_html_anchors()\n",
    "    new_page.save_links(path, fname_links)\n",
    "new_page.clean_html_text(500, ignore=english_words[:50], rx=\"[^a-zA-Z ]+\")\n",
    "new_page.k_common_words(k=5,ignore=english_words[:50])\n",
    "' '.join(new_page.cleaned_text)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_page_all(url, k, max_words, ignore_text, ignore_common, path = None):\n",
    "    page = Webpage(url)\n",
    "    fname_text = page.hash+'.text'\n",
    "    fname_links = page.hash+'.links'\n",
    "    if path == None:\n",
    "        page.get_page_html()\n",
    "        page.get_html_text(tags=[\"p\",\"h1\",\"h2\",\"h3\",\"span\"])\n",
    "        page.get_html_anchors()\n",
    "    else:\n",
    "        if os.path.isfile(path+fname_text): \n",
    "            page.load_text(path, fname_text)\n",
    "        else:\n",
    "            page.get_page_html()\n",
    "            page.get_html_text(tags=[\"p\",\"h1\",\"h2\",\"h3\",\"span\"])\n",
    "            page.save_text(path, fname_text)\n",
    "\n",
    "        if os.path.isfile(path+fname_links): \n",
    "            page.load_links(path, fname_links)\n",
    "        else:\n",
    "            if page.html == \"\": page.get_page_html()\n",
    "            page.get_html_anchors()\n",
    "            page.save_links(path, fname_links)\n",
    "\n",
    "    if page.text is not None:\n",
    "        page.clean_html_text(max_words, ignore=ignore_text, rx=\"[^a-zA-Z ]+\")\n",
    "        page.k_common_words(k=k, ignore=ignore_common)\n",
    "    return page\n",
    "\n",
    "def get_all_links(url, dict, k, min_words=20, max_words=500, ignore_text=[], ignore_common=[], ignore_filenames=[\".mp3\",\".jpg\",\".png\"], max_links=\"\", path=None):\n",
    "    primary_page = get_page_all(url, k, max_words, ignore_text, ignore_common, path)\n",
    "    if primary_page.cleaned_text is not []:\n",
    "        dict[url] = [primary_page.cleaned_text, primary_page.most_common_words]\n",
    "        if max_links == \"\" or max_links > len(primary_page.links): max_links=len(primary_page.links)\n",
    "        \n",
    "        for count, link in enumerate(primary_page.links[:max_links]):\n",
    "            if all(x not in link for x in ignore_filenames):\n",
    "                try:\n",
    "                    page = get_page_all(link, k, max_words, ignore_text, ignore_common, path)\n",
    "                    if page.cleaned_text is not []:\n",
    "                        if len(page.cleaned_text) < min_words: continue\n",
    "                        if [page.cleaned_text, page.most_common_words] in dict.values(): continue\n",
    "                        dict[link] = [page.cleaned_text, page.most_common_words]\n",
    "                except:\n",
    "                    pass\n",
    "            if link in dict:\n",
    "                res = str(len(dict[link][0]))+\" words | \"+str(dict[link][1][:3])\n",
    "            else:\n",
    "                res = \"Rejected\"\n",
    "            progress_message = \"%s link %4d/%4d | %s = %s %s\" % (url, count, len(primary_page.links), link, res, 200*' ')\n",
    "            sys.stdout.write(\"\\r\" + progress_message)\n",
    "            sys.stdout.flush()\n",
    "    else:\n",
    "        print(url,\"returned None, Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30 # words\n",
    "min_words = 50\n",
    "max_words = 450\n",
    "max_links = 100\n",
    "ignore_text = ['the', 'of', 'to', 'and', 'a', 'in', 'it', 'that', 'for', 'on'] \n",
    "ignore_common = english_words[:50]\n",
    "ignore_filenames = [\".mp3\",\".jpg\",\".png\",\".mp4\",\".jfif\",\"facebook.com\",\"twitter.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://apnews.com/ link   21/  22 | https://www.ap.org/careers/ = 156 words | [('our', 11), ('us', 3), ('career', 3)]                                                                                                                                                                                                                                                                                                                                        cted                                                                                                                                                                                                         "
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "d_pse = {}\n",
    "d_sci = {}\n",
    "path = os.getcwd()+'/data/'\n",
    "if os.path.isdir(path) is False: os.mkdir(path)\n",
    "path_pse = path+'pseudoscience/'\n",
    "path_sci = path+'science/'\n",
    "if os.path.isdir(path_pse) is False: os.mkdir(path_pse)\n",
    "if os.path.isdir(path_sci) is False: os.mkdir(path_sci)\n",
    "\n",
    "for source in pseudo_sources:\n",
    "    get_all_links(source, d_pse, k, min_words, max_words, ignore_text, ignore_common, \n",
    "    ignore_filenames, max_links, path_pse)\n",
    "for source in science_sources:\n",
    "    get_all_links(source, d_sci, k, min_words, max_words, ignore_text, ignore_common, \n",
    "    ignore_filenames, max_links, path_sci)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Pseudoscience 30 Most Common Words ####\n",
      " [('our', 1150), ('health', 715), ('their', 669), ('food', 621), ('more', 600), ('has', 597), ('us', 495), ('about', 469), ('its', 415), ('will', 397), ('mother', 391), ('my', 387), ('policy', 336), ('her', 313), ('who', 303), ('information', 285), ('news', 285), ('tweet', 279), ('if', 274), ('people', 258), ('new', 257), ('share', 250), ('email', 239), ('subscribe', 234), ('been', 232), ('heartland', 226), ('which', 222), ('no', 220), ('pin', 216), ('after', 204)] \n",
      "\n",
      "\n",
      "#### Science 30 Most Common Words ####\n",
      " [('medicine', 906), ('more', 808), ('our', 784), ('about', 760), ('science', 735), ('health', 619), ('menu', 485), ('climate', 460), ('research', 452), ('us', 426), ('information', 378), ('care', 373), ('access', 372), ('news', 368), ('has', 350), ('its', 334), ('new', 328), ('will', 328), ('nature', 325), ('johns', 324), ('these', 302), ('used', 295), ('their', 277), ('cookies', 275), ('if', 267), ('may', 249), ('storage', 233), ('technical', 230), ('close', 225), ('been', 221)] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "count_pse = Counter()\n",
    "count_sci = Counter()\n",
    "for link in d_pse:\n",
    "    count_pse+=Counter(dict(d_pse[link][1]))\n",
    "for link in d_sci:\n",
    "    count_sci+=Counter(dict(d_sci[link][1]))\n",
    "\n",
    "print(\"#### Pseudoscience\",k,\"Most Common Words ####\\n\",count_pse.most_common(k),\"\\n\\n\")\n",
    "print(\"#### Science\",k,\"Most Common Words ####\\n\",count_sci.most_common(k),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for link in d_pse:\n",
    "    if link in d_sci.keys():\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>common_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://www.ageofautism.com/</th>\n",
       "      <td>thanks you we hit our goal can not thank you enough those who donated those who read us those who share our posts those who contributed over last years most all our benefactors who so generously matched ill let you secret we are what is known tax parlance as post card organization does not mean we vacation so often we send postcards weekly means we are small but mighty our tax return fits post card feel energized grateful losing was earth shattering he was our our journalism genius our kindhearted rapier witted rebel alliance leader knew instinctively story autism was profoundly important ...</td>\n",
       "      <td>our who twitter deaths those so during percent content autism lies age been cause unknown people than see policy goal thank us tax post card mean story even has aware</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.ageofautism.com/donate.html</th>\n",
       "      <td>hello your donation autism age is tax now use secure online donations scroll down their easy use form you can always send us paper or electronic check as well email me at any time with ideas suggestions or gentle critiques our is thank you ed cause unknown epidemic sudden deaths health defense transcend fear blueprint mindful leadership public health jr real bill gates big global war democracy public health health defense donate click cover buy book shop amazon support recent comments past current contributors connect search donate donate top</td>\n",
       "      <td>health donate defense public hello donation autism age tax now secure online donations scroll down their easy form always send us paper electronic check well email me any time ideas</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.ageofautism.com/contact-us.html</th>\n",
       "      <td>autism age box ct ed cause unknown epidemic sudden deaths health defense transcend fear blueprint mindful leadership public health jr real bill gates big global war democracy public health health defense donate click cover buy book shop amazon support recent comments past current contributors connect search donate contact us top</td>\n",
       "      <td>health defense public donate autism age box ct ed cause unknown epidemic sudden deaths transcend fear blueprint mindful leadership jr real bill gates big global war democracy click cover buy</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.ageofautism.com/exclusives.html</th>\n",
       "      <td>editorials from series by here come you had me at an elaborate fraud series deer special report what do epidemiological studies really tell us note from there are epidemiological studies here vaccines autism these studies represent most often cited papers by scientists public health officials members media when trying refute any evidence an association between vaccinations autism there are serious methodological limitations design flaws conflicts interest or other problems related each these studies these flaws have been pointed out by government officials other researchers medical review ...</td>\n",
       "      <td>autism studies health these series here public epidemiological vaccines officials limitations flaws study defense donate editorials come me elaborate fraud deer special report do really tell us note represent most</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.ageofautism.com/science/</th>\n",
       "      <td>tom urged get their bivalent vaccine booster yesterday twitter kindly let him his followers know about week class starting this week through countermeasures injury compensation program this is program absolves corporations whose products harm during pandemic you can take course both live recorded version national vaccine injury compensation program countermeasures injury compensation program used emergency authorized this course students will learn structure function defects programs us created by congress award compensation adults children potentially actually harmed or killed by vaccines...</td>\n",
       "      <td>compensation injury program vaccine course high disease consequence no countermeasures us four their know week will john march health before been infectious has get class through products both live national</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                text  \\\n",
       "http://www.ageofautism.com/                  thanks you we hit our goal can not thank you enough those who donated those who read us those who share our posts those who contributed over last years most all our benefactors who so generously matched ill let you secret we are what is known tax parlance as post card organization does not mean we vacation so often we send postcards weekly means we are small but mighty our tax return fits post card feel energized grateful losing was earth shattering he was our our journalism genius our kindhearted rapier witted rebel alliance leader knew instinctively story autism was profoundly important ...   \n",
       "https://www.ageofautism.com/donate.html                                                         hello your donation autism age is tax now use secure online donations scroll down their easy use form you can always send us paper or electronic check as well email me at any time with ideas suggestions or gentle critiques our is thank you ed cause unknown epidemic sudden deaths health defense transcend fear blueprint mindful leadership public health jr real bill gates big global war democracy public health health defense donate click cover buy book shop amazon support recent comments past current contributors connect search donate donate top   \n",
       "https://www.ageofautism.com/contact-us.html                                                                                                                                                                                                                                                                               autism age box ct ed cause unknown epidemic sudden deaths health defense transcend fear blueprint mindful leadership public health jr real bill gates big global war democracy public health health defense donate click cover buy book shop amazon support recent comments past current contributors connect search donate contact us top   \n",
       "https://www.ageofautism.com/exclusives.html  editorials from series by here come you had me at an elaborate fraud series deer special report what do epidemiological studies really tell us note from there are epidemiological studies here vaccines autism these studies represent most often cited papers by scientists public health officials members media when trying refute any evidence an association between vaccinations autism there are serious methodological limitations design flaws conflicts interest or other problems related each these studies these flaws have been pointed out by government officials other researchers medical review ...   \n",
       "https://www.ageofautism.com/science/         tom urged get their bivalent vaccine booster yesterday twitter kindly let him his followers know about week class starting this week through countermeasures injury compensation program this is program absolves corporations whose products harm during pandemic you can take course both live recorded version national vaccine injury compensation program countermeasures injury compensation program used emergency authorized this course students will learn structure function defects programs us created by congress award compensation adults children potentially actually harmed or killed by vaccines...   \n",
       "\n",
       "                                                                                                                                                                                                                                                      common_words  \\\n",
       "http://www.ageofautism.com/                                                                 our who twitter deaths those so during percent content autism lies age been cause unknown people than see policy goal thank us tax post card mean story even has aware   \n",
       "https://www.ageofautism.com/donate.html                                      health donate defense public hello donation autism age tax now secure online donations scroll down their easy form always send us paper electronic check well email me any time ideas   \n",
       "https://www.ageofautism.com/contact-us.html                         health defense public donate autism age box ct ed cause unknown epidemic sudden deaths transcend fear blueprint mindful leadership jr real bill gates big global war democracy click cover buy   \n",
       "https://www.ageofautism.com/exclusives.html  autism studies health these series here public epidemiological vaccines officials limitations flaws study defense donate editorials come me elaborate fraud deer special report do really tell us note represent most   \n",
       "https://www.ageofautism.com/science/                compensation injury program vaccine course high disease consequence no countermeasures us four their know week will john march health before been infectious has get class through products both live national   \n",
       "\n",
       "                                                     label  \n",
       "http://www.ageofautism.com/                  pseudoscience  \n",
       "https://www.ageofautism.com/donate.html      pseudoscience  \n",
       "https://www.ageofautism.com/contact-us.html  pseudoscience  \n",
       "https://www.ageofautism.com/exclusives.html  pseudoscience  \n",
       "https://www.ageofautism.com/science/         pseudoscience  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "d_all = {}\n",
    "for link in d_pse:\n",
    "    text = d_pse[link][0]\n",
    "    if len(text) > max_words: text = text[:max_words]\n",
    "    common_words = ' '.join([count[0] for count in d_pse[link][1]])\n",
    "    if link not in d_all:\n",
    "        d_all[link] = [' '.join(text), common_words, 'pseudoscience']\n",
    "\n",
    "for link in d_sci:\n",
    "    text = d_sci[link][0]\n",
    "    if len(text) > max_words: text = text[:max_words]\n",
    "    common_words = ' '.join([count[0] for count in d_sci[link][1]])\n",
    "    if link not in d_all:\n",
    "        d_all[link] = [' '.join(text), common_words, 'science']\n",
    "\n",
    "df = pd.DataFrame.from_dict(d_all, orient='index', columns=['text', 'common_words', 'label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos this exchange consists seven posts each with comment by reply by plus one other post by is translation mostly by google translate from original posts comments have kindly xxunk translation you they are welcome comment at xxunk thumb keep mind posts these xxunk comments were written two years ago thus can not take more recent comments here into account we hope publish sections seven consecutive we chose day week without remembering its connection ancient days readers these authors are invited contribute further comments our comment system this weeks post starts with an introduction by followed by first his posts at comment by followed by xxunk by by two years ago professors informatics respectively published an article titled using statistical methods model molecular machines systems journal theoretical biology regular scientific intelligent design i d community publication was seen as breakthrough i d close reading paper however is not convincing seven</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos this is an additional post by made at site after seven posts his breakthrough intelligent design series were made he has requested we post here after second post series explain his position post what can be considered legitimate science central point discussion have with posts entitled breakthrough design concerns limits what can be considered legitimate science since his comments this matter are too long comments section have made post subject central question is how formulate hypothesis is suggested explanation hypotheses are not formulated an xxunk way you just grab something air hypothesis should be based prior knowledge should also be falsifiable must be possible show is incorrect scientific work then consists hypothesis whether proves be true or false something new has been learned thing here is hypothesis is formulated such way can be tested with result if you can not do you can not learn anything new if test</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos this blog ran from march has been replaced by no armed guards xxunk no raising smart kids ct no unwanted xxunk no xxunk design no autism no profiling cold reading no jun experiences xxunk enemies reason xxunk amusement parks mystery park closed no may peter evolution bacterial xxunk chiropractors run xxunk cam aids soldiers forced work as male xxunk st no report tam xxunk skeptical journalists no science religion politics no mar bigots begins filming what will become its hit show paranormal state ted hypocrisy psychic xxunk without clue critical thinking ct xxunk politics science young earth creationists poll reveals superstitions mercury health healing prayer studies find people who pray are talking themselves mice no march mostly republican war science no march abortion zoo takes intelligent design association advancement science xxunk educational practices no xxunk over cartoons no bizarre case tale torture murder rape xxunk satanic xxunk xxunk films</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = TextDataLoaders.from_df(df, bs=16, text_col='text', label_col='label')\n",
    "dls.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.569296</td>\n",
       "      <td>0.391617</td>\n",
       "      <td>0.829384</td>\n",
       "      <td>00:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.429256</td>\n",
       "      <td>0.315014</td>\n",
       "      <td>0.876777</td>\n",
       "      <td>01:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.364461</td>\n",
       "      <td>0.349550</td>\n",
       "      <td>0.862559</td>\n",
       "      <td>01:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.239097</td>\n",
       "      <td>0.230481</td>\n",
       "      <td>0.914692</td>\n",
       "      <td>01:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.160992</td>\n",
       "      <td>0.202744</td>\n",
       "      <td>0.914692</td>\n",
       "      <td>01:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "learn.fine_tune(4, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Device 0 | NVIDIA GeForce GTX 1050 Ti | Allocated: 0.4 GB | Cached: 2.1 GB\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos mother illustration meeting which explained trump planned declare victory election night regardless actual results was not supposed be big deal gathering was intended help associates his patron xxunk mogul plan election night coverage news one media companies but wanted talk he explained repeatedly trump would announce early he had won would suggest any apparent victory had resulted from fraud he s just gon na go this is just with like percent vote counted trumps just gon na walk go winner said also took credit xxunk trump into position execute strategy saying several times he had xxunk plan arrested momentum late campaign was leak material from hunter laptop was very close winning outright then shutting down trump said hard drive from hell stopped his momentum drove up his xxunk argued failure respond allegations about hunter including false ones had driven up negative perceptions former vice president compared hunter material release</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos walker speaks during campaign rally john not too long ago walker said he had experienced racism having been stopped harassed by police because he was black man he expressed his fear police will abuse his son because he s black yet during his campaign senate walker republican now runoff against democratic incumbent has repeatedly declared racism does not exist united states this is all part confusing series remarks about racism walker has made recent years year before he would announce his senate bid walker appeared podcast jr black conservative discussed assorted matters relating race referring his son walker noted he was afraid if police stop him because he s black moreover walker recalled his own xxunk xxunk with law enforcement officers been stopped by police been harassed by police before discussing one incident when he was pulled over by cops he said why world are you gon na stop</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos organic vegan xxunk organic plant based protein powder with ingredients your body will thank you add scoop smoothies overnight xxunk or baking need recipe go just add water sweet smooth way prevent pm crash free shipping within us this was reaction when she tried chocolate protein how are we able get rid xxunk texture xxunk flavor you get from other protein powders simple we use unique blend plant proteins one main ingredients is organic pea protein but instead turning whole pea pod all into pea protein we xxunk first this makes our powder smooth delicious avoids additives good reason with only ingredients be wondering how does taste so good answer we use real food keep pure no fillers no preservatives no artificial colors no artificial sweeteners we also avoid instead we use organic xxunk fruit add just right amount sweetness view nutrition facts view nutrition facts going double check</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos by daily mail est updated est as one most anticipated events global fashion calendar fashion councils annual awards ceremony is xxunk biggest names industry but with only few days left before this years xxunk event at royal hall dark disturbing cloud has been cast over proceedings scandal which no doubt has left both guests xxunk at their xxunk xxunk among xxunk famous xxunk xxunk scoop gong designer year no less is none other than creative director luxury fashion brand which this week was midst scandal over its use images children references child pornography controversy was sparked by its advertising campaign which showed xxunk clutching bags disguised as teddy bears what has been described as bondage gear as if this creepy enough separate spring campaign featuring xxunk neither whom had any creative input features xxunk placed top legal documents relating child pornography shot there is also book about controversial artist</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos stock ready ship verified customer team member hi ever since we launched our small company have wanted make snack bar would be happy eat you see always been type person reward myself at end long day with little treat was ritual i d grab my favorite cup tea treat then i d sit couch enjoy was little me moment would help me unwind from day problem sometimes would eat piece chocolate other times was another snack would feel good moment horrible later needed something better why set out create something better wanted snack bar snack bar could xxunk eat at end long day feel good about only had one question what makes great snack bar had rules ever eat snack bar seemingly xxunk all xxunk out your mouth as if you spent an eternity desert or worse bar so sticky you feel like you need wash your hands immediately</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos about this rating we received reader mail asked if copying pasting text from post could help show more content from friends also remove ads posts which we refer as claimed posting text would upgrade system help regain friends your news feed get rid ads however this was hoax most posts we found looked like this posts read as follows regain friends your news feed get rid ads hold your finger anywhere this post click copy go your page where says what s your mind tap your finger anywhere blank field click paste this upgrades system hello new old friends we previously reported other variations this same hoax one such post from claimed copying pasting its text could help avoid hearing from same friends nobody else another one promised copying pasting characters could circumvent algorithm make old friends appear users feeds neither these posts was true published an article about</td>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos march moment science my focus is somewhere neighborhood xxunk today probably because made mistake reading news about xxunk at everything anyway spirit not being able xxunk here are random xxunk science keep me xxunk at night moment science hey did you know we still have widespread agreement if virus something can be dead is technically alive first place xxunk infectious proteins can cause neurological disorders are simultaneously not alive damn near impossible kill largest living organism is mushroom makes blue xxunk look like xxunk genetic tests conducted samples from all over ton fungus covering xxunk forest confirmed was indeed one is considered delicious if not mildly poisonous when cooked xxunk we know what color dinosaurs were do with information what you will but xxunk xxunk xxunk xxunk xxunk looks pretty when its about feet below sea level bring surface where has no business hanging out looks well way you</td>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos xxunk jr xxunk onto stage at southern church radiating confidence xxunk standing xxunk crowd with his xxunk blue bobby eyes then he launched into an rant democrats drank he told people assembled far right conference branded as standing health freedom is criminal medical xxunk give child one these vaccines xxunk according video event one his many assertions ignored or went against legal scientific public health consensus then xxunk his book if just attendees amazon night he told crowd would land bestseller list they could stick amazon all profits he said would go his charity health defense while many nonprofits businesses have struggled during pandemic group has xxunk an investigation by associated press finds health defense has xxunk funding followers as used his star power as member one most famous families open doors raise money lend his group credibility filings with charity regulators show revenue more than doubled million since</td>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xxbos who we are our website address what personal data we collect why we collect comments when visitors leave comments site we collect data shown comments form also visitors address browser user agent string help spam detection an string created from your email address also called xxunk may be provided service see if you are using service privacy policy is available here after approval your comment your profile picture is visible public context your comment media though this is not something generally happens with visitors this website bears noting if you upload images website you should avoid xxunk images with embedded location data included visitors website can download extract any location data from images website contact forms cookies if you leave comment our site you may saving your name email address website cookies these are your convenience so you do not have fill your details again when you leave another</td>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sources = {\n",
    "'https://infowarslife.com/':'pseudoscience',\n",
    "'https://www.si.edu/explore/science':'science',\n",
    "'https://www.foxnews.com/opinion':'pseudoscience',\n",
    "'https://www.theskepticsguide.org/about':'science',\n",
    "'https://www.huffpost.com/':'pseudoscience',\n",
    "'https://arstechnica.com/':'science',\n",
    "'https://newspunch.com/':'pseudoscience'}\n",
    "\n",
    "pseudo_train_sources = dict([(source,'pseudoscience') for source in pseudo_sources])\n",
    "science_train_sources = dict([(source,'science') for source in science_sources])\n",
    "test_sources.update(pseudo_train_sources)\n",
    "test_sources.update(science_train_sources)\n",
    "sources = test_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy = 0.9211404025554657\n",
      "Train Source Accuracy = 0.9328858351707459\n",
      "Test Source Accuracy = 0.8791924289294651\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>training source</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>https://infowarslife.com/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.964744</td>\n",
       "      <td>False</td>\n",
       "      <td>0.964744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.si.edu/explore/science</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.997675</td>\n",
       "      <td>False</td>\n",
       "      <td>0.997675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.foxnews.com/opinion</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.967668</td>\n",
       "      <td>False</td>\n",
       "      <td>0.967668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.theskepticsguide.org/about</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.998813</td>\n",
       "      <td>False</td>\n",
       "      <td>0.998813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.huffpost.com/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.979651</td>\n",
       "      <td>False</td>\n",
       "      <td>0.979651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://arstechnica.com/</th>\n",
       "      <td>science</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.753343</td>\n",
       "      <td>False</td>\n",
       "      <td>0.246657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://newspunch.com/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.999139</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               actual     prediction  \\\n",
       "https://infowarslife.com/               pseudoscience  pseudoscience   \n",
       "https://www.si.edu/explore/science            science        science   \n",
       "https://www.foxnews.com/opinion         pseudoscience  pseudoscience   \n",
       "https://www.theskepticsguide.org/about        science        science   \n",
       "https://www.huffpost.com/               pseudoscience  pseudoscience   \n",
       "https://arstechnica.com/                      science  pseudoscience   \n",
       "https://newspunch.com/                  pseudoscience  pseudoscience   \n",
       "\n",
       "                                        probability  training source  accuracy  \n",
       "https://infowarslife.com/                  0.964744            False  0.964744  \n",
       "https://www.si.edu/explore/science         0.997675            False  0.997675  \n",
       "https://www.foxnews.com/opinion            0.967668            False  0.967668  \n",
       "https://www.theskepticsguide.org/about     0.998813            False  0.998813  \n",
       "https://www.huffpost.com/                  0.979651            False  0.979651  \n",
       "https://arstechnica.com/                   0.753343            False  0.246657  \n",
       "https://newspunch.com/                     0.999139            False  0.999139  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_pred = {}\n",
    "\n",
    "for source in sources:\n",
    "    train_source = False\n",
    "    page = get_page_all(source, k, max_words, ignore_text, ignore_common)\n",
    "    length = len(page.cleaned_text)\n",
    "    if  length < min_words:\n",
    "        print(\"ERROR:\",source,length,\"words\")\n",
    "    else:\n",
    "        common_words = ' '.join([count[0] for count in page.most_common_words])\n",
    "        text = ' '.join(page.cleaned_text)\n",
    "        with learn.no_bar(), learn.no_logging():\n",
    "            prediction = learn.predict(text)\n",
    "        if prediction[0] == \"science\":\n",
    "            p = prediction[2][1].item()\n",
    "        else:\n",
    "            p = prediction[2][0].item()\n",
    "\n",
    "        if source in d_all.keys(): train_source = True\n",
    "        if sources[source] == prediction[0]:\n",
    "            accuracy = p\n",
    "        else:\n",
    "            accuracy = 1-p\n",
    "        d_pred[source] = [sources[source], prediction[0], p, train_source, accuracy]\n",
    "\n",
    "df = pd.DataFrame.from_dict(d_pred, orient='index', columns=['actual', 'prediction', 'probability', 'training source', 'accuracy'])\n",
    "\n",
    "avg_accuracy = df['accuracy'].mean()\n",
    "train_accuracy = df.loc[df['training source'] == True, 'accuracy'].mean()\n",
    "test_accuracy = df.loc[df['training source'] == False, 'accuracy'].mean()\n",
    "\n",
    "print(\"Average Accuracy =\",avg_accuracy)\n",
    "print(\"Train Source Accuracy =\",train_accuracy)\n",
    "print(\"Test Source Accuracy =\",test_accuracy)\n",
    "df.loc[df['training source'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting and Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.export('models/2022.12.01 Model v1 88pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = load_learner('models/2022.11.28 Model.pth', cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
