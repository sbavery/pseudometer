{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> Web scraping and tools for data collection and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sbavery/pseudometer/blob/main/nbs/01_data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import enchant\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "from fastai.text.all import *\n",
    "import hashlib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility Function to Check GPU Status\n",
    "def check_gpu():\n",
    "    print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    if num_devices > 0:\n",
    "        for device in range(0,num_devices):\n",
    "            print(\"Device\", device, \"|\", torch.cuda.get_device_name(device), \n",
    "            \"| Allocated:\", round(torch.cuda.memory_allocated(device)/1024**3,1), \"GB\",\n",
    "            \"| Cached:\", round(torch.cuda.memory_reserved(device)/1024**3,1), \"GB\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Device 0 | NVIDIA GeForce RTX 3050 Ti Laptop GPU | Allocated: 0.0 GB | Cached: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "check_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Webpage:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.hash = self.get_hash_str()\n",
    "        self.requested = False\n",
    "        self.page_text = \"\"\n",
    "        self.html = \"\"\n",
    "        self.links = []\n",
    "        self.text = []\n",
    "        self.cleaned_text = []\n",
    "        self.most_common_words = []\n",
    "    \n",
    "    def get_page(self, headers, min_size, max_size):\n",
    "        r = requests.get(self.url, stream=True, headers=headers)\n",
    "        content_length = int(r.headers.get('Content-Length', 0))\n",
    "        data = []\n",
    "        length = 0\n",
    "\n",
    "        if content_length > max_size:\n",
    "            return None\n",
    "\n",
    "        for chunk in r.iter_content(1024):\n",
    "            data.append(chunk)\n",
    "            length += len(chunk)\n",
    "            if length > max_size:\n",
    "                return None\n",
    "        r._content = b''.join(data)\n",
    "        if len(r.text) < min_size: return None\n",
    "        return r.text\n",
    "\n",
    "    def get_page_html(self, min_size=1000, max_size=2000000):\n",
    "        user_agents = [ \n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36', \n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36', \n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36', \n",
    "            'Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148', \n",
    "            'Mozilla/5.0 (Linux; Android 11; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.72 Mobile Safari/537.36' \n",
    "        ] \n",
    "        user_agent = random.choice(user_agents) \n",
    "        headers = {'User-Agent': user_agent} \n",
    "        self.page_text = self.get_page(headers, min_size, max_size)\n",
    "        self.html = BeautifulSoup(self.page_text, \"html.parser\")\n",
    "        self.requested = True\n",
    "\n",
    "    def get_hash_str(self, inp=\"\"):\n",
    "        return hashlib.sha3_256((self.url+inp).encode()).hexdigest()\n",
    "\n",
    "    def get_html_anchors(self, keyword=\"http\"):\n",
    "        for anchor in self.html.findAll('a'):\n",
    "            link = anchor.get('href')\n",
    "            if link == None or link == \"\":\n",
    "                continue\n",
    "            if keyword in link:\n",
    "                self.links.append(link)\n",
    "                \n",
    "    def get_html_text(self, tags=[\"p\"]):\n",
    "        for tag in tags:\n",
    "            for p in self.html.findAll(tag):\n",
    "                p_text = p.getText().strip()\n",
    "                if p_text == None or p_text == '':\n",
    "                    continue\n",
    "                self.text.append(p_text)\n",
    "\n",
    "    def clean_html_text(self, max_words, enchant_dict=\"en_US\", ignore=[], rx=\"[^a-zA-Z ]+\", min_word_len=2):\n",
    "        all_text = ' '.join(self.text).lower()\n",
    "        regex_text = re.sub(rx,'',all_text).strip()\n",
    "        split = regex_text.split()\n",
    "        split = [word for word in split if word not in ignore]\n",
    "        if enchant_dict != \"\": d = enchant.Dict(enchant_dict)\n",
    "        for word in split:\n",
    "            if len(self.cleaned_text) >= max_words: break\n",
    "            if len(word) >= min_word_len:\n",
    "                if enchant_dict == \"\":\n",
    "                    self.cleaned_text.append(word)\n",
    "                elif d.check(word): \n",
    "                    self.cleaned_text.append(word)\n",
    "\n",
    "    def k_common_words(self, k=10, ignore=[]):\n",
    "        if self.cleaned_text == \"\":\n",
    "            text = self.text\n",
    "        else:\n",
    "            text = self.cleaned_text\n",
    "        all_text = ' '.join(text).lower()\n",
    "        split = all_text.split()\n",
    "        split_ignore = [word for word in split if word not in ignore]\n",
    "        counts = Counter(split_ignore)\n",
    "        k_most_common = counts.most_common(k)\n",
    "        self.most_common_words = k_most_common\n",
    "\n",
    "    def save_text(self, path, fname):\n",
    "        file = open(path+fname, 'wb')\n",
    "        pickle.dump(self.text, file)\n",
    "        file.close()\n",
    "\n",
    "    def load_text(self, path, fname):\n",
    "        file = open(path+fname, 'rb')\n",
    "        self.text = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    def save_links(self, path, fname):\n",
    "        file = open(path+fname, 'wb')\n",
    "        pickle.dump(self.links, file)\n",
    "        file.close()\n",
    "\n",
    "    def load_links(self, path, fname):\n",
    "        file = open(path+fname, 'rb')\n",
    "        self.links = pickle.load(file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 most common English words\n"
     ]
    }
   ],
   "source": [
    "url = \"https://gist.githubusercontent.com/deekayen/4148741/raw/98d35708fa344717d8eee15d11987de6c8e26d7d/1-1000.txt\"\n",
    "common_english = Webpage(url)\n",
    "common_english.get_page_html(min_size=1000)\n",
    "english_words = common_english.html.getText().lower()\n",
    "english_words = english_words.split('\\n')\n",
    "print(len(english_words),\"most common English words\")\n",
    "#english_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_sources = [\"http://www.ageofautism.com/\",\n",
    " \"http://www.naturalnews.com\", \n",
    " \"https://foodbabe.com/starthere/\",\n",
    " \"http://www.chopra.com\",\n",
    " \"https://www.mercola.com/\",\n",
    " \"https://www.history.com/\",\n",
    " \"https://doctoroz.com/\",\n",
    " \"https://www.disclose.tv/\",\n",
    " \"https://nationalreport.net/\",\n",
    " \"https://heartland.org/\",\n",
    " \"https://www.dailymail.co.uk/\",\n",
    " \"https://www.motherjones.com/\"]\n",
    "\n",
    "science_sources = [\"https://sciencebasedmedicine.org/\",\n",
    " \"https://www.hopkinsmedicine.org/gim/research/method/ebm.html\",\n",
    " \"https://www.bbc.com/news/science_and_environment\",\n",
    " \"https://www.nature.com/\",\n",
    " \"https://www.science.org/\",\n",
    " \"https://www.snopes.com/top/\",\n",
    " \"https://quackwatch.org/\",\n",
    " \"https://www.skepdic.com/\",\n",
    " \"http://scibabe.com/\",\n",
    " \"http://pandasthumb.org/\",\n",
    " \"https://skepticalscience.com/\",\n",
    " \"https://www.cdc.gov/\",\n",
    " \"https://apnews.com/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = science_sources[7]\n",
    "path = os.getcwd()+'/data/'\n",
    "if os.path.isdir(path) is False: os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_page = Webpage(url)\n",
    "test_page.get_page_html()\n",
    "test_page.get_html_text()\n",
    "test_page.get_html_anchors()\n",
    "test_page.clean_html_text(500, ignore=english_words[:50], rx=\"[^a-zA-Z ]+\")\n",
    "test_page.save_text(path, test_page.hash+'.text')\n",
    "test_page.save_links(path, test_page.hash+'.links')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Text\n",
      "Loading Links\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'skeptics dictionary features definitions arguments essays hundreds strange beliefs amusing deceptions dangerous delusions also features dozens entries logical fallacies cognitive biases perception science philosophy also posted over years reader comments date status entry reader comments natural cancer cures revision argument ignorance reader comments reader comments psychokinesis reader comments sample skeptics dictionary grotto river gave near peasant named claimed virgin identifying herself immaculate conception appeared her think such great number provided opportunity channel short theological treatise significance seems however main message alleged mother god pray do penance conversion world take drink spring nutshell gods beings unnatural powers who never die believed controllers creators various parts nature many thought require worship obedience humans these gods reward punish us depending whether please them stories gods been told most societies know going back least years gods portrayed beings who never die mighty powers able make nature do want most gods pictured being born having parents stories show gods being able change weather cause mighty storms floods earthquakes volcanic eruptions most part scientists replaced stories about gods scientific explanations universe formed humans living creatures belief omnipotent omniscient creator world does itself any moral still decide whether right obey if human beings anything special creatures must ponder talk jay jay essay innocently lack conflict between science religion arises lack overlap between their respective domains professional empirical constitution universe religion search proper ethical values spiritual meaning our lives attainment wisdom full life requires extensive attention both great book tells us truth make us free will live optimal harmony our fellows learn do justly love mercy walk hell broke jay essay innocently lack conflict between science religion arises lack overlap between their respective domains professional empirical constitution universe religion search proper ethical values spiritual meaning our lives attainment wisdom full life requires extensive attention both great book tells us truth make us free will live optimal harmony our fellows learn do justly love mercy walk hell broke jay essay innocently wrote lack conflict between science religion arises lack overlap between their respective domains professional empirical constitution universe religion search proper ethical values spiritual meaning our lives attainment wisdom full life requires extensive attention both great book tells us truth make us free will live optimal harmony our fellows learn do justly love mercy walk humbly then hell broke books ordering information print versions available dutch'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_page = Webpage(url)\n",
    "fname_text = new_page.hash+'.text'\n",
    "fname_links = new_page.hash+'.links'\n",
    "if os.path.isfile(path+fname_text): \n",
    "    new_page.load_text(path, fname_text)\n",
    "    print(\"Loading Text\")\n",
    "else:\n",
    "    new_page.get_page_html()\n",
    "    new_page.get_html_text(tags=[\"p\",\"h1\",\"h2\",\"h3\",\"span\"])\n",
    "    new_page.save_text(path, fname_text)\n",
    "\n",
    "if os.path.isfile(path+fname_links): \n",
    "    new_page.load_links(path, fname_links)\n",
    "    print(\"Loading Links\")\n",
    "else:\n",
    "    new_page.get_page_html()\n",
    "    new_page.get_html_anchors()\n",
    "    new_page.save_links(path, fname_links)\n",
    "new_page.clean_html_text(500, ignore=english_words[:50], rx=\"[^a-zA-Z ]+\")\n",
    "new_page.k_common_words(k=5,ignore=english_words[:50])\n",
    "' '.join(new_page.cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_page_all(url, k, max_words, ignore_text, ignore_common, path = None):\n",
    "    page = Webpage(url)\n",
    "    fname_text = page.hash+'.text'\n",
    "    fname_links = page.hash+'.links'\n",
    "    if path == None:\n",
    "        page.get_page_html()\n",
    "        page.get_html_text(tags=[\"p\",\"h1\",\"h2\",\"h3\",\"span\"])\n",
    "        page.get_html_anchors()\n",
    "    else:\n",
    "        if os.path.isfile(path+fname_text): \n",
    "            page.load_text(path, fname_text)\n",
    "        else:\n",
    "            page.get_page_html()\n",
    "            page.get_html_text(tags=[\"p\",\"h1\",\"h2\",\"h3\",\"span\"])\n",
    "            page.save_text(path, fname_text)\n",
    "\n",
    "        if os.path.isfile(path+fname_links): \n",
    "            page.load_links(path, fname_links)\n",
    "        else:\n",
    "            if page.html == \"\": page.get_page_html()\n",
    "            page.get_html_anchors()\n",
    "            page.save_links(path, fname_links)\n",
    "\n",
    "    if page.text is not None:\n",
    "        page.clean_html_text(max_words, ignore=ignore_text, rx=\"[^a-zA-Z ]+\")\n",
    "        page.k_common_words(k=k, ignore=ignore_common)\n",
    "    return page\n",
    "\n",
    "def get_all_links(url, dict, k, min_words=20, max_words=500, ignore_text=[], ignore_common=[], ignore_filenames=[\".mp3\",\".jpg\",\".png\"], max_links=\"\", path=None):\n",
    "    primary_page = get_page_all(url, k, max_words, ignore_text, ignore_common, path)\n",
    "    if primary_page.cleaned_text is not []:\n",
    "        dict[url] = [primary_page.cleaned_text, primary_page.most_common_words]\n",
    "        if max_links == \"\" or max_links > len(primary_page.links): max_links=len(primary_page.links)\n",
    "        \n",
    "        for count, link in enumerate(primary_page.links[:max_links]):\n",
    "            if all(x not in link for x in ignore_filenames):\n",
    "                try:\n",
    "                    page = get_page_all(link, k, max_words, ignore_text, ignore_common, path)\n",
    "                    if page.cleaned_text is not []:\n",
    "                        if len(page.cleaned_text) < min_words: continue\n",
    "                        if [page.cleaned_text, page.most_common_words] in dict.values(): continue\n",
    "                        dict[link] = [page.cleaned_text, page.most_common_words]\n",
    "                except:\n",
    "                    pass\n",
    "            if link in dict:\n",
    "                res = str(len(dict[link][0]))+\" words | \"+str(dict[link][1][:3])\n",
    "            else:\n",
    "                res = \"Rejected\"\n",
    "            progress_message = \"%s link %4d/%4d | %s = %s %s\" % (url, count, len(primary_page.links), link, res, 200*' ')\n",
    "            sys.stdout.write(\"\\r\" + progress_message)\n",
    "            sys.stdout.flush()\n",
    "    else:\n",
    "        print(url,\"returned None, Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30 # words\n",
    "min_words = 50\n",
    "max_words = 450\n",
    "max_links = 100\n",
    "ignore_text = ['the', 'of', 'to', 'and', 'a', 'in', 'it', 'that', 'for', 'on'] \n",
    "ignore_common = english_words[:50]\n",
    "ignore_filenames = [\".mp3\",\".jpg\",\".png\",\".mp4\",\".jfif\",\"facebook.com\",\"twitter.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#d_pse = {}\n",
    "#get_all_links(pseudo_sources[2], d_pse, k, min_text_len, ignore_words, ignore_filenames)\n",
    "#d_pse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://apnews.com/ link   20/  21 | https://www.ap.org/careers/ = 156 words | [('our', 11), ('us', 3), ('career', 3)]                                                                                                                                                                                                                                                                                                                          ion.png = Rejected                                                                                                                                                                                                         "
     ]
    }
   ],
   "source": [
    "d_pse = {}\n",
    "d_sci = {}\n",
    "path = os.getcwd()+'/data/'\n",
    "if os.path.isdir(path) is False: os.mkdir(path)\n",
    "path_pse = path+'pseudoscience/'\n",
    "path_sci = path+'science/'\n",
    "if os.path.isdir(path_pse) is False: os.mkdir(path_pse)\n",
    "if os.path.isdir(path_sci) is False: os.mkdir(path_sci)\n",
    "\n",
    "for source in pseudo_sources:\n",
    "    get_all_links(source, d_pse, k, min_words, max_words, ignore_text, ignore_common, \n",
    "    ignore_filenames, max_links, path_pse)\n",
    "for source in science_sources:\n",
    "    get_all_links(source, d_sci, k, min_words, max_words, ignore_text, ignore_common, \n",
    "    ignore_filenames, max_links, path_sci)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Pseudoscience 30 Most Common Words ####\n",
      " [('our', 1139), ('health', 711), ('more', 625), ('food', 620), ('has', 578), ('their', 550), ('about', 502), ('us', 465), ('my', 411), ('will', 409), ('mother', 400), ('her', 399), ('its', 379), ('who', 340), ('policy', 334), ('information', 289), ('news', 287), ('email', 283), ('tweet', 281), ('new', 276), ('share', 267), ('after', 261), ('if', 254), ('been', 253), ('people', 235), ('subscribe', 233), ('heartland', 226), ('twitter', 222), ('no', 216), ('pin', 216)] \n",
      "\n",
      "\n",
      "#### Science 30 Most Common Words ####\n",
      " [('medicine', 906), ('more', 792), ('our', 783), ('about', 760), ('science', 725), ('health', 618), ('menu', 483), ('us', 429), ('research', 425), ('climate', 418), ('information', 395), ('care', 373), ('news', 357), ('access', 349), ('new', 336), ('its', 332), ('has', 324), ('johns', 324), ('will', 320), ('nature', 310), ('these', 293), ('used', 290), ('their', 286), ('cookies', 274), ('if', 259), ('may', 258), ('storage', 228), ('technical', 225), ('no', 224), ('general', 215)] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_pse = Counter()\n",
    "count_sci = Counter()\n",
    "for link in d_pse:\n",
    "    count_pse+=Counter(dict(d_pse[link][1]))\n",
    "for link in d_sci:\n",
    "    count_sci+=Counter(dict(d_sci[link][1]))\n",
    "\n",
    "print(\"#### Pseudoscience\",k,\"Most Common Words ####\\n\",count_pse.most_common(k),\"\\n\\n\")\n",
    "print(\"#### Science\",k,\"Most Common Words ####\\n\",count_sci.most_common(k),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in d_pse:\n",
    "    if link in d_sci.keys():\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>common_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://www.ageofautism.com/</th>\n",
       "      <td>during spring kvetched twitter had slapped sensitive content label all our tweets included link or photo was before musk stepped see imagine if twitter became open again hope twitter exposes all censorship so many us have faced especially crackdown mean want see overtly controversial despicable content allowed but who defines pornography right know when see from defender twitter ditches misinformation policy plans reveal internal files free speech suppression effective twitter is no longer enforcing misleading information policy company said adding will soon reveal internal files free spee...</td>\n",
       "      <td>twitter content policy information so who misinformation which accounts analysis see enforcing misleading twitters about post deaths age autism our tweets included photo musk many us crackdown reveal internal files</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.ageofautism.com/donate.html</th>\n",
       "      <td>hello your donation autism age is tax now use secure online donations scroll down their easy use form you can always send us paper or electronic check as well email me at any time with ideas suggestions or gentle critiques our is thank you ed cause unknown epidemic sudden deaths health defense transcend fear blueprint mindful leadership public health jr real bill gates big global war democracy public health health defense donate click cover buy book shop amazon support recent comments past current contributors connect search donate donate top</td>\n",
       "      <td>health donate defense public hello donation autism age tax now secure online donations scroll down their easy form always send us paper electronic check well email me any time ideas</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.ageofautism.com/contact-us.html</th>\n",
       "      <td>autism age box ct ed cause unknown epidemic sudden deaths health defense transcend fear blueprint mindful leadership public health jr real bill gates big global war democracy public health health defense donate click cover buy book shop amazon support recent comments past current contributors connect search donate contact us top</td>\n",
       "      <td>health defense public donate autism age box ct ed cause unknown epidemic sudden deaths transcend fear blueprint mindful leadership jr real bill gates big global war democracy click cover buy</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.ageofautism.com/exclusives.html</th>\n",
       "      <td>editorials from series by here come you had me at an elaborate fraud series deer special report what do epidemiological studies really tell us note from there are epidemiological studies here vaccines autism these studies represent most often cited papers by scientists public health officials members media when trying refute any evidence an association between vaccinations autism there are serious methodological limitations design flaws conflicts interest or other problems related each these studies these flaws have been pointed out by government officials other researchers medical review ...</td>\n",
       "      <td>autism studies health these series here public epidemiological vaccines officials limitations flaws study defense donate editorials come me elaborate fraud deer special report do really tell us note represent most</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.ageofautism.com/science/</th>\n",
       "      <td>tom urged get their bivalent vaccine booster yesterday twitter kindly let him his followers know about week class starting this week through countermeasures injury compensation program this is program absolves corporations whose products harm during pandemic you can take course both live recorded version national vaccine injury compensation program countermeasures injury compensation program used emergency authorized this course students will learn structure function defects programs us created by congress award compensation adults children potentially actually harmed or killed by vaccines...</td>\n",
       "      <td>compensation injury program vaccine course high disease consequence no countermeasures us four their know week will john march health before been infectious has get class through products both live national</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                text  \\\n",
       "http://www.ageofautism.com/                  during spring kvetched twitter had slapped sensitive content label all our tweets included link or photo was before musk stepped see imagine if twitter became open again hope twitter exposes all censorship so many us have faced especially crackdown mean want see overtly controversial despicable content allowed but who defines pornography right know when see from defender twitter ditches misinformation policy plans reveal internal files free speech suppression effective twitter is no longer enforcing misleading information policy company said adding will soon reveal internal files free spee...   \n",
       "https://www.ageofautism.com/donate.html                                                         hello your donation autism age is tax now use secure online donations scroll down their easy use form you can always send us paper or electronic check as well email me at any time with ideas suggestions or gentle critiques our is thank you ed cause unknown epidemic sudden deaths health defense transcend fear blueprint mindful leadership public health jr real bill gates big global war democracy public health health defense donate click cover buy book shop amazon support recent comments past current contributors connect search donate donate top   \n",
       "https://www.ageofautism.com/contact-us.html                                                                                                                                                                                                                                                                               autism age box ct ed cause unknown epidemic sudden deaths health defense transcend fear blueprint mindful leadership public health jr real bill gates big global war democracy public health health defense donate click cover buy book shop amazon support recent comments past current contributors connect search donate contact us top   \n",
       "https://www.ageofautism.com/exclusives.html  editorials from series by here come you had me at an elaborate fraud series deer special report what do epidemiological studies really tell us note from there are epidemiological studies here vaccines autism these studies represent most often cited papers by scientists public health officials members media when trying refute any evidence an association between vaccinations autism there are serious methodological limitations design flaws conflicts interest or other problems related each these studies these flaws have been pointed out by government officials other researchers medical review ...   \n",
       "https://www.ageofautism.com/science/         tom urged get their bivalent vaccine booster yesterday twitter kindly let him his followers know about week class starting this week through countermeasures injury compensation program this is program absolves corporations whose products harm during pandemic you can take course both live recorded version national vaccine injury compensation program countermeasures injury compensation program used emergency authorized this course students will learn structure function defects programs us created by congress award compensation adults children potentially actually harmed or killed by vaccines...   \n",
       "\n",
       "                                                                                                                                                                                                                                                       common_words  \\\n",
       "http://www.ageofautism.com/                  twitter content policy information so who misinformation which accounts analysis see enforcing misleading twitters about post deaths age autism our tweets included photo musk many us crackdown reveal internal files   \n",
       "https://www.ageofautism.com/donate.html                                       health donate defense public hello donation autism age tax now secure online donations scroll down their easy form always send us paper electronic check well email me any time ideas   \n",
       "https://www.ageofautism.com/contact-us.html                          health defense public donate autism age box ct ed cause unknown epidemic sudden deaths transcend fear blueprint mindful leadership jr real bill gates big global war democracy click cover buy   \n",
       "https://www.ageofautism.com/exclusives.html   autism studies health these series here public epidemiological vaccines officials limitations flaws study defense donate editorials come me elaborate fraud deer special report do really tell us note represent most   \n",
       "https://www.ageofautism.com/science/                 compensation injury program vaccine course high disease consequence no countermeasures us four their know week will john march health before been infectious has get class through products both live national   \n",
       "\n",
       "                                                     label  \n",
       "http://www.ageofautism.com/                  pseudoscience  \n",
       "https://www.ageofautism.com/donate.html      pseudoscience  \n",
       "https://www.ageofautism.com/contact-us.html  pseudoscience  \n",
       "https://www.ageofautism.com/exclusives.html  pseudoscience  \n",
       "https://www.ageofautism.com/science/         pseudoscience  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_all = {}\n",
    "for link in d_pse:\n",
    "    text = d_pse[link][0]\n",
    "    if len(text) > max_words: text = text[:max_words]\n",
    "    common_words = ' '.join([count[0] for count in d_pse[link][1]])\n",
    "    if link not in d_all:\n",
    "        d_all[link] = [' '.join(text), common_words, 'pseudoscience']\n",
    "\n",
    "for link in d_sci:\n",
    "    text = d_sci[link][0]\n",
    "    if len(text) > max_words: text = text[:max_words]\n",
    "    common_words = ' '.join([count[0] for count in d_sci[link][1]])\n",
    "    if link not in d_all:\n",
    "        d_all[link] = [' '.join(text), common_words, 'science']\n",
    "\n",
    "df = pd.DataFrame.from_dict(d_all, orient='index', columns=['text', 'common_words', 'label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos this exchange consists seven posts each with comment by reply by plus one other post by is translation mostly by google translate from original posts comments have kindly corrected translation you they are welcome comment at xxunk thumb keep mind posts these xxunk comments were written two years ago thus can not take more recent comments here into account we hope publish sections seven consecutive we chose day week without remembering its connection ancient days readers these authors are invited contribute further comments our comment system this weeks post starts with an introduction by followed by first his posts at comment by followed by xxunk by by two years ago professors informatics respectively published an article titled using statistical methods model molecular machines systems journal theoretical biology regular scientific intelligent design i d community publication was seen as breakthrough i d close reading paper however is not convincing seven</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos walker speaks during campaign rally john not too long ago walker said he had experienced racism having been stopped harassed by police because he was black man he expressed his fear police will abuse his son because he s black yet during his campaign senate walker republican now runoff against democratic xxunk has repeatedly declared racism does not exist united states this is all part confusing series remarks about racism walker has made recent years year before he would announce his senate bid walker appeared podcast jr black conservative discussed assorted matters relating race referring his son walker noted he was afraid if police stop him because he s black moreover walker xxunk his own troubling xxunk with law enforcement officers been stopped by police been harassed by police before discussing one incident when he was pulled over by cops he said why world are you gon na stop</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos this is an additional post by made at site after seven posts his breakthrough intelligent design series were made he has requested we post here after second post series explain his position post what can be considered legitimate science central point discussion have with posts entitled breakthrough design concerns limits what can be considered legitimate science since his comments this matter are too long comments section have made post subject central question is how formulate hypothesis is suggested explanation hypotheses are not formulated an xxunk way you just grab something air hypothesis should be based prior knowledge should also be falsifiable must be possible show is incorrect scientific work then consists hypothesis whether proves be true or false something new has been learned thing here is hypothesis is formulated such way can be tested with result if you can not do you can not learn anything new if test</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = TextDataLoaders.from_df(df, bs=16, text_col='text', label_col='label')\n",
    "dls.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.537045</td>\n",
       "      <td>0.454569</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.408883</td>\n",
       "      <td>0.262582</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.344620</td>\n",
       "      <td>0.299470</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.230716</td>\n",
       "      <td>0.226557</td>\n",
       "      <td>0.919048</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.143907</td>\n",
       "      <td>0.223225</td>\n",
       "      <td>0.938095</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "learn.fine_tune(4, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Device 0 | NVIDIA GeForce RTX 3050 Ti Laptop GPU | Allocated: 0.4 GB | Cached: 2.2 GB\n"
     ]
    }
   ],
   "source": [
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos by weekends my husband have simple routine whoever xxunk up first gets ready day means taking him bathroom getting him changed giving him his while one us tends other gets bit break ease into day its sweet relationship my husband have father xxunk son son looks up father they need me intervene or xxunk way their weekend routine one recent morning though when got up later than my xxunk heard take his xxunk eggs xxunk cup coffee its breakfast make almost every day gets same sans coffee been awake about minutes by time i d gotten our meals ready last morning assuming my husband had already gotten his morning hovered over my chair just about sit down eat gotten good night sleep night before was less than xxunk morning so xxunk out assume he s just waiting his meal right my husband apologized said no he still needs take</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos heartland institute submitted public comments proposal repeal clean power plan environmental protect agency issued an advanced notice proposed titled repeal carbon dioxide emission guidelines existing xxunk sources electric utility generating units clean power plan heartland institute senior fellow peter research fellow submitted comprehensive xxunk documented more than figures more than xxunk public comment support repeal they note comment addresses following topics about heartland institute clean power plan is based an xxunk interpretation section clean air consequently there is no legal authority must be costs have already vastly exceeded even expected benefits ii fossil fuels are essential prosperity worldwide hundreds years since industrial revolution fossil fuel use is has been associated with higher economic growth xxunk wages health life xxunk population reduced even after decades government xxunk xxunk alternative energy sources such as solar wind play only niche role us energy official us government projections show fossil fuels will be</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos ca nt get enough my writing want find out what else been up around internet well here talks appearances guest appearances detection fall food babe interview with thinking atheist when good science sounds agenda agenda avoiding food myths bad agenda ten questions food agenda things xxunk us reason rally experience dumb food myths debunked by this should vaccines be freedom report fake news survival guide hosted by atheist community business being atheists national convention how live life atheists national convention everything is killing you marketing guide bad science skeptical runs skeptical eye over alternative national science week people need hear from agriculture food babe vs vs princess brings scientific xxunk freedom report vaccines skeptical libertarian xxunk read between headlines xxunk sponsored by communicating about food myths social media age animal agricultural alliance food glorious with logic global warming evolution vaccines more beer xxunk national science week promo with xxunk</td>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos please read these terms conditions carefully before using website terms conditions contain important information about your rights obligations as well as limitations exclusions by using this website fullest extent permitted by law you confirm your xxunk agreement acceptance these terms conditions if you xxunk not accept these terms conditions do not use this website all content this website is subject change at any time without notice attempts ensure information this website is complete accurate current despite our efforts information this website may occasionally be inaccurate incomplete or out fullest extent permitted by law makes no representation as how complete accurate or current any information is this website may make changes its website design functionality content at any time may provide links other sites are not maintained by does not endorse those sites is not responsible content such other sites please read our policy governing such third party content may</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos by weekends my husband have simple routine whoever xxunk up first gets ready day means taking him bathroom getting him changed giving him his while one us tends other gets bit break ease into day its sweet relationship my husband have father xxunk son son looks up father they need me intervene or xxunk way their weekend routine one recent morning though when got up later than my xxunk heard take his xxunk eggs xxunk cup coffee its breakfast make almost every day gets same sans coffee been awake about minutes by time i d gotten our meals ready last morning assuming my husband had already gotten his morning hovered over my chair just about sit down eat gotten good night sleep night before was less than xxunk morning so xxunk out assume he s just waiting his meal right my husband apologized said no he still needs take</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos stock ready ship verified customer team member hi ever since we launched our small company have wanted make snack bar would be happy eat you see always been type person reward myself at end long day with little treat was ritual i d grab my favorite cup tea treat then i d sit xxunk enjoy was little me moment would help me unwind from day problem sometimes would eat piece chocolate other times was another snack would feel good moment horrible later needed something better why set out create something better wanted snack bar snack bar could xxunk eat at end long day feel good about only had one question what makes great snack bar had rules ever eat snack bar seemingly xxunk all xxunk out your mouth as if you spent an eternity desert or worse bar so sticky you feel like you need wash your hands immediately</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos prince attended game against heat at garden night they were pictured sitting xxunk cheering team just one hour after princes prize launch was xxunk into palace race row reverend was one several speakers at launch where she made speech included comments impact racism climate change her comments came amid controversy sparked by own godmother lady who was accused making racist comments domestic abuse campaigner night however put brave face game during which they sat owner his disgraced former fumed over media speculation about his his fellow executives lifestyle an interview xxunk medias metric is clicks during phone call with an xxunk from call he could be heard saying how he was tired speculation about his his fellow executives love lives earlier this month was revealed one his companies wrote about being according reports his xxunk roommates at headquarters were all dating each other officials are scratching their heads after</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos by associated press est updated est jury has convicted man murder pike county massacre more than six years after eight members another family were slaughtered their sleep iv was found guilty all counts he faced southern pike county including eight counts aggravated murder shootings seven adults teenager from family sat motionless as verdicts were read closing his eyes or looking down jurors delivered verdict after weighing denials other testimony against word witnesses including his brother mother who previously pleaded guilty their roles prosecutors said family at time massacre became obsessed with getting custody over then daughter with after they feared child would be molested by new boyfriend denied any knowledge his involvement killings testified he have let happen if he had known plans fatal shootings at three mobile homes xxunk near terrified residents launched one states most extensive criminal investigations prosecutors say slayings which initially spurred speculation about drug</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xxbos xxunk jr xxunk onto stage at southern church radiating confidence xxunk standing xxunk crowd with his xxunk blue xxunk eyes then he launched into an rant democrats drank he told people assembled far right conference branded as standing health freedom is criminal medical xxunk give child one these vaccines xxunk according video event one his many assertions ignored or went against legal scientific public health consensus then xxunk his book if just attendees amazon night he told crowd would land bestseller list they could stick amazon all profits he said would go his charity health defense while many nonprofits businesses have struggled during pandemic group has xxunk an investigation by associated press finds health defense has xxunk funding followers as used his star power as member one most famous families open doors raise money lend his group credibility filings with charity regulators show revenue more than doubled million since</td>\n",
       "      <td>science</td>\n",
       "      <td>pseudoscience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sources = {'https://infowarslife.com/':'pseudoscience',\n",
    "'https://www.dailymail.co.uk/':'pseudoscience',\n",
    "'https://apnews.com/':'science',\n",
    "'https://www.si.edu/explore/science':'science',\n",
    "'https://www.foxnews.com/opinion':'pseudoscience',\n",
    "'https://www.disclose.tv/':'pseudoscience',\n",
    "'https://www.snopes.com/top/':'science',\n",
    "'https://www.theskepticsguide.org/about':'science',\n",
    "'https://www.cdc.gov/':'science',\n",
    "'https://www.motherjones.com/':'pseudoscience',\n",
    "'https://www.huffpost.com/':'pseudoscience',\n",
    "'https://arstechnica.com/':'science',\n",
    "'https://nationalreport.net/':'pseudoscience',\n",
    "'https://newspunch.com/':'pseudoscience'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>https://infowarslife.com/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.814715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.dailymail.co.uk/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.987088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://apnews.com/</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.870788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.si.edu/explore/science</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.959251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.foxnews.com/opinion</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.996972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.disclose.tv/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.642757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.snopes.com/top/</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.991956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.theskepticsguide.org/about</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.991945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.cdc.gov/</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.995555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.motherjones.com/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.992036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.huffpost.com/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.988643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://arstechnica.com/</th>\n",
       "      <td>science</td>\n",
       "      <td>science</td>\n",
       "      <td>0.737392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://nationalreport.net/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>0.994528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://newspunch.com/</th>\n",
       "      <td>pseudoscience</td>\n",
       "      <td>science</td>\n",
       "      <td>0.566743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               actual     prediction  \\\n",
       "https://infowarslife.com/               pseudoscience  pseudoscience   \n",
       "https://www.dailymail.co.uk/            pseudoscience  pseudoscience   \n",
       "https://apnews.com/                           science        science   \n",
       "https://www.si.edu/explore/science            science        science   \n",
       "https://www.foxnews.com/opinion         pseudoscience  pseudoscience   \n",
       "https://www.disclose.tv/                pseudoscience  pseudoscience   \n",
       "https://www.snopes.com/top/                   science        science   \n",
       "https://www.theskepticsguide.org/about        science        science   \n",
       "https://www.cdc.gov/                          science        science   \n",
       "https://www.motherjones.com/            pseudoscience  pseudoscience   \n",
       "https://www.huffpost.com/               pseudoscience  pseudoscience   \n",
       "https://arstechnica.com/                      science        science   \n",
       "https://nationalreport.net/             pseudoscience  pseudoscience   \n",
       "https://newspunch.com/                  pseudoscience        science   \n",
       "\n",
       "                                        probability  \n",
       "https://infowarslife.com/                  0.814715  \n",
       "https://www.dailymail.co.uk/               0.987088  \n",
       "https://apnews.com/                        0.870788  \n",
       "https://www.si.edu/explore/science         0.959251  \n",
       "https://www.foxnews.com/opinion            0.996972  \n",
       "https://www.disclose.tv/                   0.642757  \n",
       "https://www.snopes.com/top/                0.991956  \n",
       "https://www.theskepticsguide.org/about     0.991945  \n",
       "https://www.cdc.gov/                       0.995555  \n",
       "https://www.motherjones.com/               0.992036  \n",
       "https://www.huffpost.com/                  0.988643  \n",
       "https://arstechnica.com/                   0.737392  \n",
       "https://nationalreport.net/                0.994528  \n",
       "https://newspunch.com/                     0.566743  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_pred = {}\n",
    "\n",
    "for source in test_sources:\n",
    "    page = get_page_all(source, k, max_words, ignore_text, ignore_common)\n",
    "    length = len(page.cleaned_text)\n",
    "    if  length < min_words:\n",
    "        print(\"ERROR:\",source,length,\"words\")\n",
    "    else:\n",
    "        common_words = ' '.join([count[0] for count in page.most_common_words])\n",
    "        text = ' '.join(page.cleaned_text)\n",
    "        with learn.no_bar(), learn.no_logging():\n",
    "            prediction = learn.predict(text)\n",
    "        if prediction[0] == \"science\":\n",
    "            p = prediction[2][1].item()\n",
    "        else:\n",
    "            p = prediction[2][0].item()\n",
    "        d_pred[source] = [test_sources[source], prediction[0], p]\n",
    "\n",
    "df = pd.DataFrame.from_dict(d_pred, orient='index', columns=['actual', 'prediction', 'probability'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.save('2022.11.30 Model v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = load_learner('models/2022.11.28 Model.pth', cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pseudometer",
   "language": "python",
   "name": "pseudometer"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
